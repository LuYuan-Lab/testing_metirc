import argparse
import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import cv2
import numpy as np
import torch
from PIL import Image
from torchvision import transforms
from tqdm import tqdm

from model.ResNetModel import R2Dmodel
from tool.dataset import VideoDataset
from tool.video_crop_processor import AutoCropper


class SequenceVideoPredictor:
    """ä¸“æ³¨äºè§†é¢‘åºåˆ—æ£€æµ‹çš„é¢„æµ‹å™¨"""

    def __init__(
        self,
        model_path: str,
        reference_embeddings_path: str,
        yolo_model_path: str = "weights/yolov11n.pt",
        embedding_dim: int = 128,
        device: str = "auto",
    ):
        """
        åˆå§‹åŒ–åºåˆ—é¢„æµ‹å™¨

        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡è·¯å¾„
            reference_embeddings_path: å‚è€ƒç‰¹å¾å‘é‡è·¯å¾„
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
            embedding_dim: ç‰¹å¾å‘é‡ç»´åº¦
            device: è®¡ç®—è®¾å¤‡
        """
        # è®¾ç½®è®¾å¤‡
        self.device = (
            torch.device("cuda" if torch.cuda.is_available() else "cpu") if device == "auto" else torch.device(device)
        )
        print(f"Using device: {self.device}")

        # åŠ è½½è¡Œä¸ºè¯†åˆ«æ¨¡å‹
        self.model = R2Dmodel(embedding_dim=embedding_dim)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()

        # åŠ è½½YOLOæ£€æµ‹å™¨
        self.detector = AutoCropper(model_path=yolo_model_path, conf_thres=0.25, target_class="person")
        print("âœ… YOLO detector loaded successfully")

        # åŠ è½½å‚è€ƒç‰¹å¾å‘é‡
        self._load_reference_embeddings(reference_embeddings_path)

        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose(
            [
                transforms.Resize((112, 112)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]
        )

    def _load_reference_embeddings(self, path: str):
        """åŠ è½½å‚è€ƒç‰¹å¾å‘é‡"""
        print(f"Loading reference embeddings from {path}")
        data = torch.load(path)
        self.reference_embeddings = data["embeddings"].to(self.device)
        self.reference_labels = data["labels"].to(self.device)

        # è·å–ç±»åˆ«åç§°
        temp_dataset = VideoDataset(data_root="data", mode="train", num_frames=30)
        class_names = list(temp_dataset.class_to_idx.keys())

        # ä¸­æ–‡åˆ°è‹±æ–‡çš„æ˜ å°„
        display_names = {
            "æ­£å¸¸": "Normal",
            "ä¸¾æ‰‹": "Hand Raise",
            "æ‰‹æœº": "Phone",
            "ç«™ç«‹": "Standing",
            "å·¦å³çœ‹": "Looking Around",
        }

        self.class_names = [display_names.get(name, name) for name in class_names]
        print(f"Loaded {len(self.reference_embeddings)} reference embeddings")
        print(f"Classes: {self.class_names}")

    def _filter_detection_boxes(
        self, detection_results, iou_threshold: float = 0.5
    ) -> List[Tuple[np.ndarray, float, int]]:
        """
        è¿‡æ»¤æ£€æµ‹æ¡†ï¼Œå»é™¤é‡å¤æ¡†ä½†ä¿ç•™å¤šä¸ªç›®æ ‡

        Args:
            detection_results: YOLOæ£€æµ‹ç»“æœ
            iou_threshold: IoUé˜ˆå€¼ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºåŒä¸€ç›®æ ‡

        Returns:
            è¿‡æ»¤åçš„æ£€æµ‹æ¡†åˆ—è¡¨ [(bbox, confidence, track_id), ...]
        """
        all_boxes = []
        all_confidences = []

        # æ”¶é›†æ‰€æœ‰personç±»åˆ«çš„æ£€æµ‹æ¡†
        for result in detection_results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    if int(box.cls) == 0:  # personç±»åˆ«
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        conf = float(box.conf[0].cpu().numpy())
                        all_boxes.append([x1, y1, x2, y2])
                        all_confidences.append(conf)

        if len(all_boxes) == 0:
            return []

        all_boxes = np.array(all_boxes)
        all_confidences = np.array(all_confidences)

        # åº”ç”¨è‡ªå®šä¹‰NMSï¼Œä¿ç•™å¤šä¸ªç›®æ ‡
        filtered_indices = self._multi_target_nms(all_boxes, all_confidences, iou_threshold)

        # æ„å»ºç»“æœ
        filtered_boxes = []
        for i, idx in enumerate(filtered_indices):
            bbox = all_boxes[idx].astype(int)
            confidence = all_confidences[idx]
            track_id = i  # ç®€å•çš„è·Ÿè¸ªIDåˆ†é…
            filtered_boxes.append((bbox, confidence, track_id))

        return filtered_boxes

    def _multi_target_nms(self, boxes: np.ndarray, confidences: np.ndarray, iou_threshold: float) -> List[int]:
        """
        å¤šç›®æ ‡éæœ€å¤§æŠ‘åˆ¶ç®—æ³•
        ä¸ä¼ ç»ŸNMSä¸åŒï¼Œè¿™ä¸ªç®—æ³•ä¼šä¿ç•™ä¸åŒä½ç½®çš„å¤šä¸ªç›®æ ‡
        """
        if len(boxes) == 0:
            return []

        # è®¡ç®—é¢ç§¯
        areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])

        # æŒ‰ç½®ä¿¡åº¦æ’åº
        order = confidences.argsort()[::-1]

        keep = []
        while len(order) > 0:
            # å–ç½®ä¿¡åº¦æœ€é«˜çš„æ¡†
            i = order[0]
            keep.append(i)

            if len(order) == 1:
                break

            # è®¡ç®—ä¸å…¶ä»–æ¡†çš„IoU
            ious = self._compute_iou(boxes[i], boxes[order[1:]], areas[i], areas[order[1:]])

            # ä¿ç•™IoUå°äºé˜ˆå€¼çš„æ¡†ï¼ˆä¸åŒç›®æ ‡ï¼‰
            inds = np.where(ious <= iou_threshold)[0]
            order = order[inds + 1]

        return keep

    def _compute_iou(self, box1: np.ndarray, boxes: np.ndarray, area1: float, areas: np.ndarray) -> np.ndarray:
        """è®¡ç®—å•ä¸ªæ¡†ä¸å¤šä¸ªæ¡†çš„IoU"""
        # è®¡ç®—äº¤é›†
        xx1 = np.maximum(box1[0], boxes[:, 0])
        yy1 = np.maximum(box1[1], boxes[:, 1])
        xx2 = np.minimum(box1[2], boxes[:, 2])
        yy2 = np.minimum(box1[3], boxes[:, 3])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        intersection = w * h

        # è®¡ç®—å¹¶é›†
        union = area1 + areas - intersection

        # è®¡ç®—IoU
        ious = intersection / (union + 1e-8)
        return ious

    def predict_sequence(
        self,
        video_path: str,
        segment_duration: float = 3.0,
        overlap_ratio: float = 0.3,
        frames_per_segment: int = 30,
        confidence_threshold: float = 0.4,
        max_segments: Optional[int] = None,
    ) -> Dict:
        """
        å¯¹è§†é¢‘è¿›è¡Œåºåˆ—é¢„æµ‹

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            segment_duration: æ¯ä¸ªæ—¶é—´æ®µçš„æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            overlap_ratio: ç›¸é‚»æ—¶é—´æ®µçš„é‡å æ¯”ä¾‹ï¼ˆ0-1ï¼‰
            frames_per_segment: æ¯ä¸ªæ—¶é—´æ®µé‡‡æ ·çš„å¸§æ•°
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            max_segments: æœ€å¤§æ®µæ•°é™åˆ¶ï¼ˆç”¨äºæµ‹è¯•ï¼‰

        Returns:
            åŒ…å«åºåˆ—é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Could not open video file: {video_path}")

        # è·å–è§†é¢‘ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        video_duration = total_frames / fps

        print(f"Video info: {total_frames} frames, {fps:.1f} FPS, {video_duration:.1f}s duration")
        print(f"Segment settings: {segment_duration}s per segment, {frames_per_segment} frames per segment")

        # è®¡ç®—æ—¶é—´æ®µå‚æ•°
        frames_per_segment_actual = int(segment_duration * fps)
        overlap_frames = int(frames_per_segment_actual * overlap_ratio)
        stride_frames = frames_per_segment_actual - overlap_frames

        # æ£€æµ‹ç»Ÿä¸€çš„è£å‰ªåŒºåŸŸ
        crop_rect = self._detect_crop_region(cap, total_frames)

        # ç”Ÿæˆæ—¶é—´æ®µ
        segments = self._generate_segments(total_frames, frames_per_segment_actual, stride_frames, fps)

        if max_segments and len(segments) > max_segments:
            segments = segments[:max_segments]
            print(f"Limited to first {max_segments} segments for testing")

        # é¢„æµ‹æ¯ä¸ªæ®µè½
        predictions = self._predict_segments(cap, segments, frames_per_segment, crop_rect, confidence_threshold)

        cap.release()

        # ç”Ÿæˆç»“æœ
        result = {
            "video_path": video_path,
            "video_info": {"duration": video_duration, "fps": fps, "total_frames": total_frames},
            "segment_settings": {
                "segment_duration": segment_duration,
                "frames_per_segment": frames_per_segment,
                "overlap_ratio": overlap_ratio,
                "confidence_threshold": confidence_threshold,
            },
            "summary": self._generate_summary(predictions, len(segments)),
            "predictions": predictions,
        }

        return result

    def _detect_crop_region(self, cap, total_frames):
        """æ£€æµ‹ç»Ÿä¸€çš„è£å‰ªåŒºåŸŸ"""
        print("Detecting unified crop region...")
        sample_frames = min(20, total_frames)
        sample_indices = np.linspace(0, total_frames - 1, sample_frames, dtype=int)

        all_crops = []
        for idx in sample_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # ä½¿ç”¨YOLOæ£€æµ‹å¹¶è¿‡æ»¤æ£€æµ‹æ¡†
                detection_results = self.detector.model(rgb_frame, conf=0.25, verbose=False)
                filtered_boxes = self._filter_detection_boxes(detection_results, iou_threshold=0.5)

                # å¦‚æœæœ‰å¤šä¸ªç›®æ ‡ï¼Œé€‰æ‹©é¢ç§¯æœ€å¤§çš„ï¼ˆé€šå¸¸æ˜¯ä¸»è¦ç›®æ ‡ï¼‰
                if filtered_boxes:
                    if len(filtered_boxes) == 1:
                        bbox, _, _ = filtered_boxes[0]
                        detected_crop = tuple(bbox)
                        all_crops.append(detected_crop)
                    else:
                        # å¤šä¸ªç›®æ ‡æ—¶ï¼Œé€‰æ‹©é¢ç§¯æœ€å¤§çš„ä½œä¸ºä¸»è¦è£å‰ªåŒºåŸŸ
                        best_bbox = None
                        max_area = 0
                        for bbox in filtered_boxes:
                            area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
                            if area > max_area:
                                max_area = area
                                best_bbox = bbox
                        if best_bbox is not None:
                            detected_crop = tuple(best_bbox)
                            all_crops.append(detected_crop)
                            print(
                                f"  Frame {idx}: Found {len(filtered_boxes)} targets, "
                                f"selected largest (area={max_area:.0f})"
                            )

        if all_crops:
            crop_rect = tuple(np.mean(all_crops, axis=0).astype(int))
            print(f"Unified crop region: {crop_rect}")
            return crop_rect
        else:
            print("No person detected, using full frame")
            return None

    def _generate_segments(self, total_frames, frames_per_segment_actual, stride_frames, fps):
        """ç”Ÿæˆæ—¶é—´æ®µ"""
        segments = []
        start_frame = 0
        segment_id = 0

        while start_frame + frames_per_segment_actual <= total_frames:
            end_frame = start_frame + frames_per_segment_actual
            start_time = start_frame / fps
            end_time = end_frame / fps

            segments.append(
                {
                    "id": segment_id,
                    "start_frame": start_frame,
                    "end_frame": end_frame,
                    "start_time": start_time,
                    "end_time": end_time,
                }
            )

            start_frame += stride_frames
            segment_id += 1

        print(f"Generated {len(segments)} time segments")
        return segments

    def _predict_segments(self, cap, segments, frames_per_segment, crop_rect, confidence_threshold):
        """é¢„æµ‹æ‰€æœ‰æ®µè½"""
        predictions = []

        for segment in tqdm(segments, desc="Processing segments"):
            try:
                # æå–æ®µè½å¸§
                frames = self._extract_frames(cap, segment, frames_per_segment, crop_rect)

                if len(frames) < frames_per_segment:
                    continue

                # é¢„æµ‹
                prediction = self._predict_single_segment(frames, segment, confidence_threshold)

                if prediction:
                    predictions.append(prediction)
                    print(
                        f"Segment {segment['id']:2d}: "
                        f"{segment['start_time']:5.1f}-{segment['end_time']:5.1f}s -> "
                        f"{prediction['predicted_class']:12s} ({prediction['confidence']:.3f})"
                    )
                else:
                    print(
                        f"Segment {segment['id']:2d}: "
                        f"{segment['start_time']:5.1f}-{segment['end_time']:5.1f}s -> "
                        f"Low confidence"
                    )

            except Exception as e:
                print(f"Error processing segment {segment['id']}: {e}")
                continue

        return predictions

    def _extract_frames(self, cap, segment, frames_per_segment, crop_rect):
        """ä»æ—¶é—´æ®µä¸­æå–å¸§"""
        start_frame = segment["start_frame"]
        end_frame = segment["end_frame"]

        frame_indices = np.linspace(start_frame, end_frame - 1, frames_per_segment, dtype=int)

        frames = []
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if not ret:
                continue

            # åº”ç”¨è£å‰ª
            if crop_rect is not None:
                x1, y1, x2, y2 = crop_rect
                h, w = frame.shape[:2]
                x1 = max(0, min(x1, w - 1))
                y1 = max(0, min(y1, h - 1))
                x2 = max(x1 + 1, min(x2, w))
                y2 = max(y1 + 1, min(y2, h))
                frame = frame[y1:y2, x1:x2]

            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)

        return frames

    def _predict_single_segment(self, frames, segment, confidence_threshold):
        """é¢„æµ‹å•ä¸ªæ®µè½"""
        # é¢„å¤„ç†å¸§
        processed_frames = []
        for frame in frames:
            pil_frame = Image.fromarray(frame)
            transformed = self.transform(pil_frame)
            processed_frames.append(transformed)

        # æ„é€ å¼ é‡
        video_tensor = torch.stack(processed_frames, dim=0).permute(1, 0, 2, 3).unsqueeze(0)
        video_tensor = video_tensor.to(self.device)

        # ç‰¹å¾æå–
        with torch.no_grad():
            features = self.model(video_tensor).squeeze(0)

        # ç›¸ä¼¼åº¦è®¡ç®—
        target_norm = torch.nn.functional.normalize(features.unsqueeze(0), p=2, dim=-1)
        reference_norm = torch.nn.functional.normalize(self.reference_embeddings, p=2, dim=-1)
        similarities = torch.matmul(target_norm, reference_norm.t()).squeeze(0).cpu().numpy()

        # å„ç±»åˆ«å¹³å‡ç›¸ä¼¼åº¦
        class_similarities = {}
        for class_idx, class_name in enumerate(self.class_names):
            class_mask = self.reference_labels.cpu().numpy() == class_idx
            if np.any(class_mask):
                class_similarities[class_name] = float(np.mean(similarities[class_mask]))

        # é¢„æµ‹ç±»åˆ«
        predicted_class = max(class_similarities, key=class_similarities.get)
        confidence = class_similarities[predicted_class]

        # æ£€æŸ¥ç½®ä¿¡åº¦
        if confidence >= confidence_threshold:
            return {
                "segment_id": segment["id"],
                "time_range": f"{segment['start_time']:.1f}s - {segment['end_time']:.1f}s",
                "start_time": segment["start_time"],
                "end_time": segment["end_time"],
                "predicted_class": predicted_class,
                "confidence": confidence,
                "class_similarities": class_similarities,
            }

        return None

    def _generate_summary(self, predictions, total_segments):
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        if not predictions:
            return {
                "dominant_behavior": "Unknown",
                "total_confident_predictions": 0,
                "total_segments": total_segments,
                "behavior_distribution": {},
                "average_confidence": 0.0,
                "coverage_ratio": 0.0,
            }

        # ç»Ÿè®¡è¡Œä¸ºåˆ†å¸ƒ
        class_counts = {}
        total_confidence = 0

        for pred in predictions:
            cls = pred["predicted_class"]
            class_counts[cls] = class_counts.get(cls, 0) + 1
            total_confidence += pred["confidence"]

        dominant_class = max(class_counts, key=class_counts.get)
        avg_confidence = total_confidence / len(predictions)
        coverage_ratio = len(predictions) / total_segments

        return {
            "dominant_behavior": dominant_class,
            "total_confident_predictions": len(predictions),
            "total_segments": total_segments,
            "behavior_distribution": class_counts,
            "average_confidence": avg_confidence,
            "coverage_ratio": coverage_ratio,
        }

    def create_visualization_video(self, video_path: str, sequence_results: Dict, output_path: str) -> str:
        """
        åˆ›å»ºåºåˆ—é¢„æµ‹çš„å¯è§†åŒ–è§†é¢‘

        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            sequence_results: åºåˆ—é¢„æµ‹ç»“æœ
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„

        Returns:
            è¾“å‡ºè§†é¢‘è·¯å¾„
        """
        print("ğŸ¬ Creating sequence visualization video...")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Unable to open video: {video_path}")

        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # åˆ›å»ºè¾“å‡ºè§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # è§£æé¢„æµ‹ç»“æœ
        predictions = sequence_results.get("predictions", [])
        video_info = sequence_results.get("video_info", {})

        # åˆ›å»ºæ—¶é—´åˆ°é¢„æµ‹çš„æ˜ å°„
        time_predictions = {}
        for pred in predictions:
            start_time = int(pred["start_time"])
            end_time = int(pred["end_time"])
            for t in range(start_time, end_time + 1):
                time_predictions[t] = {
                    "class": pred["predicted_class"],
                    "confidence": pred["confidence"],
                    "segment_info": pred["time_range"],
                }

        # é¢œè‰²æ˜ å°„
        color_map = {
            "Hand Raise": (0, 255, 255),  # é»„è‰²
            "Phone": (0, 0, 255),  # çº¢è‰²
            "Standing": (255, 0, 255),  # ç´«è‰²
            "Looking Around": (0, 165, 255),  # æ©™è‰²
            "Normal": (0, 255, 0),  # ç»¿è‰²
        }

        try:
            frame_count = 0
            with tqdm(total=total_frames, desc="Processing video frames") as pbar:
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break

                    current_time = int(frame_count / fps)
                    current_prediction = time_predictions.get(current_time)

                    # äººä½“æ£€æµ‹å¹¶è¿‡æ»¤æ£€æµ‹æ¡†
                    detection_results = self.detector.model(frame, conf=0.25, verbose=False)
                    filtered_boxes = self._filter_detection_boxes(detection_results, iou_threshold=0.5)

                    # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œé¢„æµ‹ç»“æœ
                    annotated_frame = frame.copy()

                    # ç»˜åˆ¶è¿‡æ»¤åçš„æ£€æµ‹æ¡†
                    for bbox, conf, track_id in filtered_boxes:
                        x1, y1, x2, y2 = bbox

                        # é€‰æ‹©é¢œè‰²
                        if current_prediction:
                            color = color_map.get(current_prediction["class"], (128, 128, 128))
                            detection_text = (
                                f"ID{track_id}: {current_prediction['class']}: {current_prediction['confidence']:.2f}"
                            )
                        else:
                            color = (128, 128, 128)
                            detection_text = f"ID{track_id}: Person: {conf:.2f}"

                        # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ–‡å­—
                        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 3)
                        cv2.putText(
                            annotated_frame, detection_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2
                        )

                    # åœ¨å³ä¸Šè§’æ˜¾ç¤ºæ£€æµ‹ç»Ÿè®¡
                    if filtered_boxes:
                        stats_text = f"Targets: {len(filtered_boxes)}"
                        cv2.putText(
                            annotated_frame,
                            stats_text,
                            (width - 150, 30),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.7,
                            (255, 255, 255),
                            2,
                        )

                    # ç»˜åˆ¶ä¿¡æ¯èƒŒæ™¯
                    cv2.rectangle(annotated_frame, (10, 10), (width - 10, 120), (0, 0, 0), -1)
                    cv2.rectangle(annotated_frame, (10, 10), (width - 10, 120), (255, 255, 255), 2)

                    # ç»˜åˆ¶æ—¶é—´å’Œé¢„æµ‹ä¿¡æ¯
                    time_text = f"Time: {current_time}s / {video_info.get('duration', 0):.0f}s"
                    cv2.putText(annotated_frame, time_text, (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

                    if current_prediction:
                        pred_text = f"Behavior: {current_prediction['class']}"
                        conf_text = f"Confidence: {current_prediction['confidence']:.3f}"
                        segment_text = f"Segment: {current_prediction['segment_info']}"

                        text_color = color_map.get(current_prediction["class"], (255, 255, 255))
                        cv2.putText(annotated_frame, pred_text, (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)
                        cv2.putText(
                            annotated_frame, conf_text, (20, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2
                        )
                        cv2.putText(
                            annotated_frame, segment_text, (20, 105), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2
                        )
                    else:
                        cv2.putText(
                            annotated_frame,
                            "Behavior: No prediction",
                            (20, 60),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.8,
                            (128, 128, 128),
                            2,
                        )

                    out.write(annotated_frame)
                    frame_count += 1
                    pbar.update(1)

        finally:
            cap.release()
            out.release()

        print(f"âœ… Sequence visualization video saved to: {output_path}")
        return output_path


def main():
    """ä¸»å‡½æ•° - ç®€åŒ–çš„å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="Video Sequence Behavior Prediction")

    # å¿…éœ€å‚æ•°
    parser.add_argument("--video_path", type=str, required=True, help="Path to video file")
    parser.add_argument("--output_dir", type=str, default="output/sequence", help="Output directory")

    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--model_path", type=str, default="checkpoints/yolodetect/best_model.pth", help="Model checkpoint path"
    )
    parser.add_argument(
        "--reference_embeddings",
        type=str,
        default="checkpoints/yolodetect/val_embeddings.pt",
        help="Reference embeddings path",
    )
    parser.add_argument("--yolo_model_path", type=str, default="weights/yolov11n.pt", help="YOLO model path")

    # åºåˆ—é¢„æµ‹å‚æ•°
    parser.add_argument("--segment_duration", type=float, default=3.0, help="Segment duration in seconds")
    parser.add_argument("--overlap_ratio", type=float, default=0.3, help="Overlap ratio between segments")
    parser.add_argument("--frames_per_segment", type=int, default=30, help="Frames per segment")
    parser.add_argument("--confidence_threshold", type=float, default=0.4, help="Confidence threshold")
    parser.add_argument("--max_segments", type=int, default=None, help="Max segments for testing")

    # å¯è§†åŒ–å‚æ•°
    parser.add_argument("--visualize", action="store_true", help="Create visualization video")

    args = parser.parse_args()

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)

    # åˆå§‹åŒ–é¢„æµ‹å™¨
    print("ğŸš€ Initializing Sequence Video Predictor...")
    predictor = SequenceVideoPredictor(
        model_path=args.model_path,
        reference_embeddings_path=args.reference_embeddings,
        yolo_model_path=args.yolo_model_path,
    )

    # è¿›è¡Œåºåˆ—é¢„æµ‹
    print(f"ğŸ¯ Starting sequence prediction on: {args.video_path}")
    result = predictor.predict_sequence(
        video_path=args.video_path,
        segment_duration=args.segment_duration,
        overlap_ratio=args.overlap_ratio,
        frames_per_segment=args.frames_per_segment,
        confidence_threshold=args.confidence_threshold,
        max_segments=args.max_segments,
    )

    # æ‰“å°ç»“æœæ‘˜è¦
    summary = result["summary"]
    print("\nğŸ“Š Prediction Summary:")
    print(f"Dominant behavior: {summary['dominant_behavior']}")
    print(f"Confident predictions: {summary['total_confident_predictions']}/{summary['total_segments']}")
    print(f"Coverage ratio: {summary['coverage_ratio']:.1%}")
    print(f"Behavior distribution: {summary['behavior_distribution']}")
    print(f"Average confidence: {summary['average_confidence']:.3f}")

    # ä¿å­˜ç»“æœ
    video_name = Path(args.video_path).stem
    json_output_path = os.path.join(args.output_dir, f"{video_name}_sequence_prediction.json")
    with open(json_output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“„ Results saved to: {json_output_path}")

    # ç”Ÿæˆå¯è§†åŒ–è§†é¢‘
    if args.visualize:
        video_output_path = os.path.join(args.output_dir, f"{video_name}_sequence_annotated.mp4")
        predictor.create_visualization_video(
            video_path=args.video_path, sequence_results=result, output_path=video_output_path
        )
        print(f"ğŸ¬ Visualization video saved to: {video_output_path}")

    print("âœ… Sequence prediction completed!")


if __name__ == "__main__":
    main()


"""
ä½¿ç”¨ç¤ºä¾‹:

# python predict.py --video_path data/test/merged_final.mp4  --visualize
"""
