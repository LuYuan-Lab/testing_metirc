"""
ç›®æ ‡æ£€æµ‹å¯è§†åŒ–å·¥å…· - ç®€åŒ–ç‰ˆ
ç›´æ¥è¿è¡Œå³å¯è¿›è¡Œç›®æ ‡æ£€æµ‹å¯è§†åŒ–å¹¶ä¿å­˜ç»“æœ

ç”¨æ³•:
python visualization/object_detection_visualization.py --input videos/111.mp4
"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path

import cv2
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tool.video_crop_processor import AutoCropper  # noqa: E402


def detect_and_visualize(
    input_video, output_dir="visualization/output_visualization/detection", confidence_threshold=0.6, max_frames=None
):
    """æ£€æµ‹è§†é¢‘å¹¶ä¿å­˜å¯è§†åŒ–ç»“æœ"""
    print(f"ğŸ” å¼€å§‹æ£€æµ‹è§†é¢‘: {input_video}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æ–‡ä»¶
    if not Path(input_video).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_video}")
        return None

    if not Path("weights/yolov11n.pt").exists():
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: weights/yolov11n.pt")
        return None

    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        detector = AutoCropper("weights/yolov11n.pt")

        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(input_video)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"ğŸ“¹ è§†é¢‘: {width}x{height}, {fps}fps, {total_frames}å¸§")

        # è¾“å‡ºæ–‡ä»¶
        video_name = Path(input_video).stem
        timestamp = datetime.now().strftime("%H%M%S")
        output_video = Path(output_dir) / f"{video_name}_detection_{timestamp}.mp4"
        output_sample = Path(output_dir) / f"{video_name}_sample_{timestamp}.jpg"

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        writer = cv2.VideoWriter(str(output_video), cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height))

        frame_count = 0
        total_detections = 0
        process_frames = min(max_frames or total_frames, total_frames)
        sample_saved = False

        while frame_count < process_frames:
            ret, frame = cap.read()
            if not ret:
                break

            # ä½¿ç”¨æ–°çš„æ£€æµ‹æ¥å£
            detections = detector.detect_with_details(
                frame=frame,
                target_class="person",
                confidence_threshold=confidence_threshold,
                return_format="detailed",
                sort_by="confidence",
            )

            # å¯è§†åŒ–ç»˜åˆ¶
            annotated = visualize_detections(frame, detections, frame_info=f"Frame: {frame_count+1}/{process_frames}")

            # ä¿å­˜
            writer.write(annotated)
            total_detections += len(detections)

            # ä¿å­˜ç¤ºä¾‹å¸§
            if not sample_saved and frame_count == process_frames // 2:
                cv2.imwrite(str(output_sample), annotated)
                sample_saved = True

            frame_count += 1

            # è¿›åº¦
            if frame_count % 50 == 0:
                progress = frame_count / process_frames * 100
                print(f"â³ è¿›åº¦: {progress:.1f}% ({frame_count}/{process_frames})")

        cap.release()
        writer.release()

        # ç»Ÿè®¡
        stats = {
            "input_video": input_video,
            "output_video": str(output_video),
            "output_sample": str(output_sample),
            "processed_frames": frame_count,
            "total_detections": total_detections,
            "avg_detections": total_detections / frame_count if frame_count > 0 else 0,
            "timestamp": timestamp,
        }

        # ä¿å­˜ç»Ÿè®¡
        stats_file = Path(output_dir) / f"{video_name}_stats_{timestamp}.json"
        with open(stats_file, "w") as f:
            json.dump(stats, f, indent=2)

        print("âœ… æ£€æµ‹å®Œæˆ!")
        print(f"ğŸ“¹ æ£€æµ‹è§†é¢‘: {output_video}")
        print(f"ğŸ“¸ ç¤ºä¾‹å›¾ç‰‡: {output_sample}")
        print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {stats_file}")
        print(f"ğŸ“ˆ å¤„ç†äº† {frame_count} å¸§ï¼Œæ£€æµ‹åˆ° {total_detections} ä¸ªç›®æ ‡")

        return stats

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return None


def visualize_detections(
    frame: np.ndarray,
    detections: list,
    frame_info: str = None,
    show_confidence: bool = True,
    show_class_name: bool = True,
    show_bbox: bool = True,
    show_center: bool = True,
    bbox_thickness: int = 2,
    font_scale: float = 0.5,
    color_scheme: str = "auto",
    transparency: float = 0.7,
):
    """
    å¯è§†åŒ–æ£€æµ‹ç»“æœçš„é€šç”¨å‡½æ•°

    Args:
        frame: è¾“å…¥å¸§
        detections: æ£€æµ‹ç»“æœåˆ—è¡¨
        frame_info: å¸§ä¿¡æ¯æ–‡æœ¬
        show_confidence: æ˜¾ç¤ºç½®ä¿¡åº¦
        show_class_name: æ˜¾ç¤ºç±»åˆ«å
        show_bbox: æ˜¾ç¤ºè¾¹æ¡†
        show_center: æ˜¾ç¤ºä¸­å¿ƒç‚¹
        bbox_thickness: è¾¹æ¡†åšåº¦
        font_scale: å­—ä½“å¤§å°
        color_scheme: é¢œè‰²æ–¹æ¡ˆ
        transparency: é€æ˜åº¦

    Returns:
        å¯è§†åŒ–åçš„å¸§
    """
    annotated = frame.copy()

    # é¢œè‰²ç”Ÿæˆ
    colors = [
        (0, 255, 0),
        (255, 0, 0),
        (0, 0, 255),
        (255, 255, 0),
        (255, 0, 255),
        (0, 255, 255),
        (128, 255, 0),
        (255, 128, 0),
        (128, 0, 255),
        (255, 0, 128),
    ]

    for i, detection in enumerate(detections):
        bbox = detection["bbox"]
        confidence = detection["confidence"]
        class_name = detection["class"]

        x1, y1, x2, y2 = map(int, bbox)
        color = colors[i % len(colors)]

        # ç»˜åˆ¶æ£€æµ‹æ¡†
        if show_bbox:
            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, bbox_thickness)

        # ç»˜åˆ¶ä¸­å¿ƒç‚¹
        if show_center:
            center = ((x1 + x2) // 2, (y1 + y2) // 2)
            cv2.circle(annotated, center, 5, color, -1)

        # æ„å»ºæ ‡ç­¾
        label_parts = []
        if show_class_name:
            label_parts.append(class_name)
        if show_confidence:
            label_parts.append(f"{confidence:.2f}")

        if label_parts:
            label = " ".join(label_parts)
            cv2.putText(annotated, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, 2)

    # ç»˜åˆ¶å¸§ä¿¡æ¯
    if frame_info:
        info_text = f"{frame_info} | Detections: {len(detections)}"
        cv2.putText(annotated, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

    return annotated


def main():
    """ä¸»å‡½æ•° - æ”¯æŒä¸°å¯Œçš„æ£€æµ‹å‚æ•°é…ç½®"""
    parser = argparse.ArgumentParser(description="ç›®æ ‡æ£€æµ‹å¯è§†åŒ–å·¥å…· - æ”¯æŒä¸°å¯Œå‚æ•°è°ƒè¯•")

    # === åŸºç¡€å‚æ•° ===
    parser.add_argument("--input", "-i", default="videos/111.mp4", help="è¾“å…¥è§†é¢‘è·¯å¾„ (é»˜è®¤: videos/111.mp4)")
    parser.add_argument("--output", "-o", default="visualization/output_visualization/detection", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max-frames", type=int, help="æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: å¤„ç†å…¨éƒ¨)")

    # === æ£€æµ‹å‚æ•° ===
    parser.add_argument("--conf", type=float, default=0.45, help="ç½®ä¿¡åº¦é˜ˆå€¼ (é»˜è®¤: 0.6)")
    parser.add_argument("--target-class", default="person", help="ç›®æ ‡ç±»åˆ« (é»˜è®¤: person, ä½¿ç”¨ all æ£€æµ‹æ‰€æœ‰)")
    parser.add_argument("--max-detections", type=int, help="æ¯å¸§æœ€å¤§æ£€æµ‹æ•°é‡ (é»˜è®¤: æ— é™åˆ¶)")
    parser.add_argument("--min-box-area", type=float, default=100, help="æœ€å°æ£€æµ‹æ¡†é¢ç§¯ (é»˜è®¤: 100)")
    parser.add_argument("--max-box-area", type=float, help="æœ€å¤§æ£€æµ‹æ¡†é¢ç§¯ (é»˜è®¤: æ— é™åˆ¶)")
    parser.add_argument("--crop-margin", type=float, default=0, help="è£å‰ªè¾¹è·åƒç´  (é»˜è®¤: 0)")
    parser.add_argument(
        "--sort-by",
        default="confidence",
        choices=["confidence", "area", "position"],
        help="æ£€æµ‹ç»“æœæ’åºæ–¹å¼ (é»˜è®¤: confidence)",
    )
    parser.add_argument("--filter-overlapping", action="store_true", help="è¿‡æ»¤é‡å æ¡† (é»˜è®¤: False)")
    parser.add_argument("--overlap-threshold", type=float, default=0.5, help="é‡å é˜ˆå€¼ (é»˜è®¤: 0.5)")

    # === å¯è§†åŒ–å‚æ•° ===
    parser.add_argument("--show-confidence", action="store_true", default=True, help="æ˜¾ç¤ºç½®ä¿¡åº¦ (é»˜è®¤: True)")
    parser.add_argument("--show-class-name", action="store_true", default=True, help="æ˜¾ç¤ºç±»åˆ«å (é»˜è®¤: True)")
    parser.add_argument("--show-center", action="store_true", default=True, help="æ˜¾ç¤ºä¸­å¿ƒç‚¹ (é»˜è®¤: True)")
    parser.add_argument("--bbox-thickness", type=int, default=2, help="è¾¹æ¡†åšåº¦ (é»˜è®¤: 2)")
    parser.add_argument("--font-scale", type=float, default=0.5, help="å­—ä½“å¤§å° (é»˜è®¤: 0.5)")
    parser.add_argument(
        "--color-scheme",
        default="auto",
        choices=["auto", "class_based", "confidence_based"],
        help="é¢œè‰²æ–¹æ¡ˆ (é»˜è®¤: auto)",
    )

    args = parser.parse_args()

    print("ğŸ” ç›®æ ‡æ£€æµ‹å¯è§†åŒ–å·¥å…·")
    print("=" * 50)
    print("ğŸ“ é…ç½®å‚æ•°:")
    print(f"   è¾“å…¥è§†é¢‘: {args.input}")
    print(f"   è¾“å‡ºç›®å½•: {args.output}")
    print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {args.conf}")
    print(f"   ç›®æ ‡ç±»åˆ«: {args.target_class}")
    print(f"   æœ€å¤§æ£€æµ‹æ•°: {args.max_detections or 'æ— é™åˆ¶'}")
    print(f"   æœ€å°é¢ç§¯: {args.min_box_area}")
    print(f"   æœ€å¤§é¢ç§¯: {args.max_box_area or 'æ— é™åˆ¶'}")
    print(f"   æ’åºæ–¹å¼: {args.sort_by}")
    print(f"   è¿‡æ»¤é‡å : {args.filter_overlapping}")
    print(f"   è¾¹æ¡†åšåº¦: {args.bbox_thickness}")
    print(f"   å­—ä½“å¤§å°: {args.font_scale}")
    print("=" * 50)

    # æ‰§è¡Œé«˜çº§æ£€æµ‹å¯è§†åŒ–
    detect_and_visualize_advanced(
        input_video=args.input,
        output_dir=args.output,
        confidence_threshold=args.conf,
        max_frames=args.max_frames,
        target_class=args.target_class,
        max_detections=args.max_detections,
        min_box_area=args.min_box_area,
        max_box_area=args.max_box_area,
        crop_margin=args.crop_margin,
        sort_by=args.sort_by,
        filter_overlapping=args.filter_overlapping,
        overlap_threshold=args.overlap_threshold,
        show_confidence=args.show_confidence,
        show_class_name=args.show_class_name,
        show_center=args.show_center,
        bbox_thickness=args.bbox_thickness,
        font_scale=args.font_scale,
        color_scheme=args.color_scheme,
    )


def detect_and_visualize_advanced(
    input_video: str,
    output_dir: str = "visualization/output_visualization/detection",
    confidence_threshold: float = 0.6,
    max_frames: int = None,
    target_class: str = "person",
    max_detections: int = None,
    min_box_area: float = 100,
    max_box_area: float = None,
    crop_margin: float = 0,
    sort_by: str = "confidence",
    filter_overlapping: bool = False,
    overlap_threshold: float = 0.5,
    show_confidence: bool = True,
    show_class_name: bool = True,
    show_center: bool = True,
    bbox_thickness: int = 2,
    font_scale: float = 0.5,
    color_scheme: str = "auto",
):
    """
    é«˜çº§æ£€æµ‹å¯è§†åŒ–å‡½æ•°ï¼Œæ”¯æŒä¸°å¯Œçš„å‚æ•°é…ç½®
    """
    print(f"ğŸ” å¼€å§‹é«˜çº§æ£€æµ‹å¤„ç†: {input_video}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æ–‡ä»¶
    if not Path(input_video).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_video}")
        return None

    if not Path("weights/yolov11n.pt").exists():
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: weights/yolov11n.pt")
        return None

    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        detector = AutoCropper("weights/yolov11n.pt")

        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(input_video)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"ğŸ“¹ è§†é¢‘: {width}x{height}, {fps}fps, {total_frames}å¸§")

        # è¾“å‡ºæ–‡ä»¶
        video_name = Path(input_video).stem
        timestamp = datetime.now().strftime("%H%M%S")
        output_video = Path(output_dir) / f"{video_name}_detection_{timestamp}.mp4"
        output_sample = Path(output_dir) / f"{video_name}_sample_{timestamp}.jpg"

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        writer = cv2.VideoWriter(str(output_video), cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height))

        frame_count = 0
        total_detections = 0
        process_frames = min(max_frames or total_frames, total_frames)
        sample_saved = False

        while frame_count < process_frames:
            ret, frame = cap.read()
            if not ret:
                break

            # ä½¿ç”¨é«˜çº§æ£€æµ‹æ¥å£
            detections = detector.detect_with_details(
                frame=frame,
                target_class=target_class if target_class != "all" else None,
                confidence_threshold=confidence_threshold,
                max_detections=max_detections,
                min_box_area=min_box_area,
                max_box_area=max_box_area,
                crop_margin=crop_margin,
                return_format="detailed",
                filter_overlapping=filter_overlapping,
                overlap_threshold=overlap_threshold,
                sort_by=sort_by,
            )

            # é«˜çº§å¯è§†åŒ–
            annotated = visualize_detections(
                frame=frame,
                detections=detections,
                frame_info=f"Frame: {frame_count+1}/{process_frames}",
                show_confidence=show_confidence,
                show_class_name=show_class_name,
                show_bbox=True,
                show_center=show_center,
                bbox_thickness=bbox_thickness,
                font_scale=font_scale,
                color_scheme=color_scheme,
            )

            # ä¿å­˜
            writer.write(annotated)
            total_detections += len(detections)

            # ä¿å­˜ç¤ºä¾‹å¸§
            if not sample_saved and frame_count == process_frames // 2:
                cv2.imwrite(str(output_sample), annotated)
                sample_saved = True

            frame_count += 1

            # è¿›åº¦
            if frame_count % 50 == 0:
                progress = frame_count / process_frames * 100
                print(f"â³ è¿›åº¦: {progress:.1f}% ({frame_count}/{process_frames}) - å½“å‰å¸§æ£€æµ‹: {len(detections)}")

        cap.release()
        writer.release()

        # ç»Ÿè®¡
        stats = {
            "input_video": input_video,
            "output_video": str(output_video),
            "output_sample": str(output_sample),
            "processed_frames": frame_count,
            "total_detections": total_detections,
            "avg_detections": total_detections / frame_count if frame_count > 0 else 0,
            "settings": {
                "confidence_threshold": confidence_threshold,
                "target_class": target_class,
                "max_detections": max_detections,
                "min_box_area": min_box_area,
                "max_box_area": max_box_area,
                "sort_by": sort_by,
                "filter_overlapping": filter_overlapping,
            },
            "timestamp": timestamp,
        }

        # ä¿å­˜ç»Ÿè®¡
        stats_file = Path(output_dir) / f"{video_name}_stats_{timestamp}.json"
        with open(stats_file, "w") as f:
            json.dump(stats, f, indent=2)

        print("âœ… æ£€æµ‹å®Œæˆ!")
        print(f"ğŸ“¹ æ£€æµ‹è§†é¢‘: {output_video}")
        print(f"ğŸ“¸ ç¤ºä¾‹å›¾ç‰‡: {output_sample}")
        print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {stats_file}")
        print(f"ğŸ“ˆ å¤„ç†äº† {frame_count} å¸§ï¼Œæ£€æµ‹åˆ° {total_detections} ä¸ªç›®æ ‡")

        return stats

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()
