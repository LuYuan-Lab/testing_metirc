"""
ç›®æ ‡è·Ÿè¸ªå¯è§†åŒ–å·¥å…· - ç®€åŒ–ç‰ˆ
ç›´æ¥è¿è¡Œå³å¯è¿›è¡Œç›®æ ‡è·Ÿè¸ªå¯è§†åŒ–å¹¶ä¿å­˜ç»“æœ

ç”¨æ³•:
python visualization/tracking_visualization.py --input videos/111.mp4
"""

import argparse
import json
import os
import sys
from collections import defaultdict, deque
from datetime import datetime
from pathlib import Path

import cv2
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tool.tracker import TrackingConfig, VideoTracker  # noqa: E402
from tool.video_crop_processor import AutoCropper  # noqa: E402


def track_and_visualize(
    input_video,
    output_dir="visualization/output_visualization/tracking",
    confidence_threshold=0.3,
    max_frames=None,
    trail_length=30,
):
    """è·Ÿè¸ªè§†é¢‘å¹¶ä¿å­˜å¯è§†åŒ–ç»“æœ"""
    print(f"ğŸ¯ å¼€å§‹è·Ÿè¸ªè§†é¢‘: {input_video}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æ–‡ä»¶
    if not Path(input_video).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_video}")
        return None

    if not Path("weights/yolov11n.pt").exists():
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: weights/yolov11n.pt")
        return None

    try:
        # åˆ›å»ºæ£€æµ‹å™¨å’Œè·Ÿè¸ªå™¨
        detector = AutoCropper("weights/yolov11n.pt")
        config = TrackingConfig(
            enable_tracking=True,
            tracker_type="bytetrack",
            track_buffer=30,
            track_low_thresh=confidence_threshold,  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„ç½®ä¿¡åº¦é˜ˆå€¼
            track_high_thresh=confidence_threshold * 1.2,  # é«˜ç½®ä¿¡åº¦é˜ˆå€¼ç¨é«˜ä¸€äº›
            new_track_thresh=confidence_threshold,  # æ–°è½¨è¿¹é˜ˆå€¼
        )
        tracker = VideoTracker(config)

        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(input_video)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"ğŸ“¹ è§†é¢‘: {width}x{height}, {fps}fps, {total_frames}å¸§")

        # è¾“å‡ºæ–‡ä»¶
        video_name = Path(input_video).stem
        timestamp = datetime.now().strftime("%H%M%S")
        output_video = Path(output_dir) / f"{video_name}_tracking_{timestamp}.mp4"
        output_sample = Path(output_dir) / f"{video_name}_sample_{timestamp}.jpg"
        output_trails = Path(output_dir) / f"{video_name}_trails_{timestamp}.jpg"

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        writer = cv2.VideoWriter(str(output_video), cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height))

        # è½¨è¿¹å­˜å‚¨
        trails = defaultdict(lambda: deque(maxlen=trail_length))
        track_ids = set()
        trails_canvas = np.zeros((height, width, 3), dtype=np.uint8)

        def get_color(track_id):
            """ä¸ºIDåˆ†é…é¢œè‰²"""
            hue = int((track_id * 137) % 180)
            color = cv2.cvtColor(np.uint8([[[hue, 255, 255]]]), cv2.COLOR_HSV2BGR)[0][0]
            return (int(color[0]), int(color[1]), int(color[2]))

        frame_count = 0
        total_detections = 0
        total_tracks = 0
        process_frames = min(max_frames or total_frames, total_frames)
        sample_saved = False

        while frame_count < process_frames:
            ret, frame = cap.read()
            if not ret:
                break

            # 1. å…ˆæ£€æµ‹
            detections = detector.detect_with_details(
                frame=frame, target_class="person", confidence_threshold=confidence_threshold, return_format="detailed"
            )

            # 2. è½¬æ¢ä¸ºè·Ÿè¸ªæ ¼å¼
            tracking_detections = []
            for det in detections:
                bbox = det["bbox"]
                conf = det["confidence"]
                tracking_detections.append((*bbox, conf, 0))  # (x1,y1,x2,y2,conf,class_id)

            # 3. æ‰§è¡Œè·Ÿè¸ª
            tracks = tracker.update(
                detections=tracking_detections,
                frame_id=frame_count,
                frame_size=(width, height),
                track_confidence_threshold=confidence_threshold,
                enable_track_smoothing=True,
            )

            # 4. å¯è§†åŒ–
            annotated = visualize_tracking(
                frame=frame,
                tracks=tracks,
                trails=trails,
                trails_canvas=trails_canvas,
                get_color_func=get_color,
                frame_info=f"Frame: {frame_count+1}",
            )

            # ç»Ÿè®¡
            current_detections = len(detections)
            current_tracks = len(tracks)
            total_detections += current_detections
            total_tracks += current_tracks

            if tracks:
                track_ids.update(tracks.keys())

            # ä¿å­˜
            writer.write(annotated)

            # ä¿å­˜ç¤ºä¾‹å¸§
            if not sample_saved and frame_count == process_frames // 2:
                cv2.imwrite(str(output_sample), annotated)
                sample_saved = True

            frame_count += 1

            # è¿›åº¦
            if frame_count % 50 == 0:
                progress = frame_count / process_frames * 100
                print(f"â³ è¿›åº¦: {progress:.1f}% ({frame_count}/{process_frames})")

        cap.release()
        writer.release()

        # ä¿å­˜è½¨è¿¹å›¾
        cv2.imwrite(str(output_trails), trails_canvas)

        # ç»Ÿè®¡
        stats = {
            "input_video": input_video,
            "output_video": str(output_video),
            "output_sample": str(output_sample),
            "output_trails": str(output_trails),
            "processed_frames": frame_count,
            "total_detections": total_detections,
            "total_tracks": total_tracks,
            "unique_ids": len(track_ids),
            "avg_detections": total_detections / frame_count if frame_count > 0 else 0,
            "avg_tracks": total_tracks / frame_count if frame_count > 0 else 0,
            "timestamp": timestamp,
        }

        # ä¿å­˜ç»Ÿè®¡
        stats_file = Path(output_dir) / f"{video_name}_stats_{timestamp}.json"
        with open(stats_file, "w") as f:
            json.dump(stats, f, indent=2)

        print("âœ… è·Ÿè¸ªå®Œæˆ!")
        print(f"ğŸ“¹ è·Ÿè¸ªè§†é¢‘: {output_video}")
        print(f"ğŸ“¸ ç¤ºä¾‹å›¾ç‰‡: {output_sample}")
        print(f"ğŸ›¤ï¸  è½¨è¿¹å›¾ç‰‡: {output_trails}")
        print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {stats_file}")
        print(
            f"ğŸ“ˆ å¤„ç†äº† {frame_count} å¸§ï¼Œæ£€æµ‹ {total_detections} æ¬¡ï¼Œè·Ÿè¸ª {total_tracks} æ¬¡ï¼Œ{len(track_ids)} ä¸ªå”¯ä¸€ID"
        )

        return stats

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return None


def visualize_tracking(
    frame: np.ndarray,
    tracks: dict,
    trails: dict,
    trails_canvas: np.ndarray,
    get_color_func,
    frame_info: str = None,
    show_trail: bool = True,
    show_velocity: bool = False,
    show_track_id: bool = True,
    trail_thickness: int = 2,
    fade_trail: bool = True,
    color_per_track: bool = True,
):
    """
    å¯è§†åŒ–è·Ÿè¸ªç»“æœçš„é€šç”¨å‡½æ•°

    Args:
        frame: è¾“å…¥å¸§
        tracks: è·Ÿè¸ªç»“æœå­—å…¸
        trails: è½¨è¿¹å†å²
        trails_canvas: è½¨è¿¹ç”»å¸ƒ
        get_color_func: è·å–é¢œè‰²çš„å‡½æ•°
        frame_info: å¸§ä¿¡æ¯
        show_trail: æ˜¾ç¤ºè½¨è¿¹
        show_velocity: æ˜¾ç¤ºé€Ÿåº¦
        show_track_id: æ˜¾ç¤ºè·Ÿè¸ªID
        trail_thickness: è½¨è¿¹åšåº¦
        fade_trail: è½¨è¿¹æ·¡åŒ–
        color_per_track: æ¯ä¸ªè½¨è¿¹ä¸åŒé¢œè‰²

    Returns:
        å¯è§†åŒ–åçš„å¸§
    """
    annotated = frame.copy()

    for track_id, track_info in tracks.items():
        if isinstance(track_info, dict):
            bbox = track_info.get("bbox")
            confidence = track_info.get("confidence", 1.0)
            track_info.get("state", "active")
        else:
            # å…¼å®¹æ—§æ ¼å¼ (x1,y1,x2,y2)
            bbox = track_info
            confidence = 1.0

        if not bbox:
            continue

        x1, y1, x2, y2 = map(int, bbox)
        center = ((x1 + x2) // 2, (y1 + y2) // 2)

        # è·å–é¢œè‰²
        color = get_color_func(track_id)

        # æ›´æ–°è½¨è¿¹
        trails[track_id].append(center)

        # ç»˜åˆ¶è½¨è¿¹
        if show_trail and len(trails[track_id]) > 1:
            for i in range(1, len(trails[track_id])):
                pt1 = trails[track_id][i - 1]
                pt2 = trails[track_id][i]

                if fade_trail:
                    alpha = i / len(trails[track_id])
                    thickness = max(1, int(trail_thickness * alpha))
                else:
                    thickness = trail_thickness

                cv2.line(annotated, pt1, pt2, color, thickness)
                cv2.line(trails_canvas, pt1, pt2, color, 2)

        # ç»˜åˆ¶æ£€æµ‹æ¡†
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
        cv2.circle(annotated, center, 5, color, -1)

        # æ ‡ç­¾
        label_parts = []
        if show_track_id:
            label_parts.append(f"ID:{track_id}")
        label_parts.append(f"{confidence:.2f}")
        if isinstance(track_info, dict) and "class" in track_info:
            label_parts.append(track_info["class"])

        label = " ".join(label_parts)
        cv2.putText(annotated, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # ç»˜åˆ¶å¸§ä¿¡æ¯
    if frame_info:
        active_tracks = [t for t in tracks.values() if isinstance(t, dict) and t.get("state") == "active"]
        info_text = f"{frame_info} | Det: {len(tracks)} | " f"Tracks: {len(active_tracks)} | IDs: {len(tracks)}"
        cv2.putText(annotated, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    return annotated


def main():
    """ä¸»å‡½æ•° - æ”¯æŒä¸°å¯Œçš„è·Ÿè¸ªå‚æ•°é…ç½®"""
    parser = argparse.ArgumentParser(description="ç›®æ ‡è·Ÿè¸ªå¯è§†åŒ–å·¥å…· - æ”¯æŒä¸°å¯Œå‚æ•°è°ƒè¯•")

    # === åŸºç¡€å‚æ•° ===
    parser.add_argument("--input", "-i", default="videos/111.mp4", help="è¾“å…¥è§†é¢‘è·¯å¾„ (é»˜è®¤: videos/111.mp4)")
    parser.add_argument("--output", "-o", default="visualization/output_visualization/tracking", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max-frames", type=int, help="æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: å¤„ç†å…¨éƒ¨)")

    # === æ£€æµ‹å‚æ•° ===
    parser.add_argument("--conf", type=float, default=0.45, help="æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ (é»˜è®¤: 0.3)")
    parser.add_argument("--target-class", default="person", help="ç›®æ ‡ç±»åˆ« (é»˜è®¤: person, ä½¿ç”¨ all æ£€æµ‹æ‰€æœ‰)")
    parser.add_argument("--max-detections", type=int, help="æ¯å¸§æœ€å¤§æ£€æµ‹æ•°é‡ (é»˜è®¤: æ— é™åˆ¶)")
    parser.add_argument("--min-box-area", type=float, default=100, help="æœ€å°æ£€æµ‹æ¡†é¢ç§¯ (é»˜è®¤: 100)")
    parser.add_argument("--max-box-area", type=float, help="æœ€å¤§æ£€æµ‹æ¡†é¢ç§¯ (é»˜è®¤: æ— é™åˆ¶)")

    # === è·Ÿè¸ªå‚æ•° ===
    parser.add_argument("--track-conf", type=float, default=0.25, help="è·Ÿè¸ªç½®ä¿¡åº¦é˜ˆå€¼ (é»˜è®¤: 0.25)")
    parser.add_argument(
        "--association-metric",
        default="cosine",
        choices=["cosine", "euclidean", "iou"],
        help="å…³è”åº¦é‡æ–¹å¼ (é»˜è®¤: cosine)",
    )
    parser.add_argument("--enable-kalman", action="store_true", default=True, help="å¯ç”¨å¡å°”æ›¼æ»¤æ³¢ (é»˜è®¤: True)")
    parser.add_argument("--max-age", type=int, default=30, help="æœ€å¤§è¿½è¸ªå¹´é¾„ (é»˜è®¤: 30)")
    parser.add_argument("--min-hits", type=int, default=3, help="æœ€å°å‘½ä¸­æ¬¡æ•° (é»˜è®¤: 3)")
    parser.add_argument("--iou-threshold", type=float, default=0.3, help="IoUé˜ˆå€¼ (é»˜è®¤: 0.3)")

    # === å¯è§†åŒ–å‚æ•° ===
    parser.add_argument("--trail-length", type=int, default=30, help="è½¨è¿¹é•¿åº¦ (é»˜è®¤: 30)")
    parser.add_argument("--show-track-id", action="store_true", default=True, help="æ˜¾ç¤ºè·Ÿè¸ªID (é»˜è®¤: True)")
    parser.add_argument("--show-confidence", action="store_true", default=True, help="æ˜¾ç¤ºç½®ä¿¡åº¦ (é»˜è®¤: True)")
    parser.add_argument("--show-trails", action="store_true", default=True, help="æ˜¾ç¤ºè¿åŠ¨è½¨è¿¹ (é»˜è®¤: True)")
    parser.add_argument("--fade-trail", action="store_true", default=True, help="æ¸å˜è½¨è¿¹ (é»˜è®¤: True)")
    parser.add_argument("--trail-thickness", type=int, default=2, help="è½¨è¿¹åšåº¦ (é»˜è®¤: 2)")
    parser.add_argument("--bbox-thickness", type=int, default=2, help="è¾¹æ¡†åšåº¦ (é»˜è®¤: 2)")
    parser.add_argument("--font-scale", type=float, default=0.5, help="å­—ä½“å¤§å° (é»˜è®¤: 0.5)")
    parser.add_argument(
        "--color-scheme",
        default="id_based",
        choices=["id_based", "confidence_based", "class_based"],
        help="é¢œè‰²æ–¹æ¡ˆ (é»˜è®¤: id_based)",
    )

    # === è¾“å‡ºå‚æ•° ===
    parser.add_argument("--save-trails", action="store_true", default=True, help="ä¿å­˜è½¨è¿¹å›¾ (é»˜è®¤: True)")
    parser.add_argument("--save-stats", action="store_true", default=True, help="ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ (é»˜è®¤: True)")

    args = parser.parse_args()

    print("ğŸ¯ ç›®æ ‡è·Ÿè¸ªå¯è§†åŒ–å·¥å…·")
    print("=" * 50)
    print("ğŸ“ é…ç½®å‚æ•°:")
    print(f"   è¾“å…¥è§†é¢‘: {args.input}")
    print(f"   è¾“å‡ºç›®å½•: {args.output}")
    print(f"   æ£€æµ‹ç½®ä¿¡åº¦: {args.conf}")
    print(f"   è·Ÿè¸ªç½®ä¿¡åº¦: {args.track_conf}")
    print(f"   ç›®æ ‡ç±»åˆ«: {args.target_class}")
    print(f"   å…³è”åº¦é‡: {args.association_metric}")
    print(f"   å¡å°”æ›¼æ»¤æ³¢: {args.enable_kalman}")
    print(f"   è½¨è¿¹é•¿åº¦: {args.trail_length}")
    print(f"   æœ€å¤§å¹´é¾„: {args.max_age}")
    print(f"   æœ€å°å‘½ä¸­: {args.min_hits}")
    print(f"   IoUé˜ˆå€¼: {args.iou_threshold}")
    print(f"   é¢œè‰²æ–¹æ¡ˆ: {args.color_scheme}")
    print("=" * 50)

    # æ‰§è¡Œé«˜çº§è·Ÿè¸ªå¯è§†åŒ–
    track_and_visualize_advanced(
        input_video=args.input,
        output_dir=args.output,
        confidence_threshold=args.conf,
        max_frames=args.max_frames,
        target_class=args.target_class,
        max_detections=args.max_detections,
        min_box_area=args.min_box_area,
        max_box_area=args.max_box_area,
        track_confidence_threshold=args.track_conf,
        association_metric=args.association_metric,
        enable_kalman_filter=args.enable_kalman,
        max_age=args.max_age,
        min_hits=args.min_hits,
        iou_threshold=args.iou_threshold,
        trail_length=args.trail_length,
        show_track_id=args.show_track_id,
        show_confidence=args.show_confidence,
        show_trails=args.show_trails,
        fade_trail=args.fade_trail,
        trail_thickness=args.trail_thickness,
        bbox_thickness=args.bbox_thickness,
        font_scale=args.font_scale,
        color_scheme=args.color_scheme,
        save_trails=args.save_trails,
        save_stats=args.save_stats,
    )


def track_and_visualize_advanced(
    input_video: str,
    output_dir: str = "visualization/output_visualization/tracking",
    confidence_threshold: float = 0.3,
    max_frames: int = None,
    target_class: str = "person",
    max_detections: int = None,
    min_box_area: float = 100,
    max_box_area: float = None,
    track_confidence_threshold: float = 0.25,
    association_metric: str = "cosine",
    enable_kalman_filter: bool = True,
    max_age: int = 30,
    min_hits: int = 3,
    iou_threshold: float = 0.3,
    trail_length: int = 30,
    show_track_id: bool = True,
    show_confidence: bool = True,
    show_trails: bool = True,
    fade_trail: bool = True,
    trail_thickness: int = 2,
    bbox_thickness: int = 2,
    font_scale: float = 0.5,
    color_scheme: str = "id_based",
    save_trails: bool = True,
    save_stats: bool = True,
):
    """
    é«˜çº§è·Ÿè¸ªå¯è§†åŒ–å‡½æ•°ï¼Œæ”¯æŒä¸°å¯Œçš„å‚æ•°é…ç½®
    """
    print(f"ğŸ¯ å¼€å§‹é«˜çº§è·Ÿè¸ªå¤„ç†: {input_video}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æ–‡ä»¶
    if not Path(input_video).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_video}")
        return None

    if not Path("weights/yolov11n.pt").exists():
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: weights/yolov11n.pt")
        return None

    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨å’Œè·Ÿè¸ªå™¨
        detector = AutoCropper("weights/yolov11n.pt")

        # åˆ›å»ºè·Ÿè¸ªé…ç½®ï¼ˆæ˜ å°„å‚æ•°åˆ°æ­£ç¡®çš„é…ç½®åç§°ï¼‰
        tracking_config = TrackingConfig(
            enable_tracking=True,
            tracker_type="bytetrack",
            track_buffer=max_age,  # ä½¿ç”¨ track_buffer æ›¿ä»£ max_age
            match_threshold=iou_threshold,  # ä½¿ç”¨ match_threshold æ›¿ä»£ iou_threshold
            min_box_area=min_box_area,
            track_high_thresh=track_confidence_threshold,  # é«˜ç½®ä¿¡åº¦é˜ˆå€¼
            track_low_thresh=track_confidence_threshold * 0.5,  # ä½ç½®ä¿¡åº¦é˜ˆå€¼
            new_track_thresh=track_confidence_threshold,  # æ–°è½¨è¿¹é˜ˆå€¼
            track_lost_thresh=max_age,  # è½¨è¿¹ä¸¢å¤±é˜ˆå€¼
        )
        tracker = VideoTracker(tracking_config)

        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(input_video)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"ğŸ“¹ è§†é¢‘: {width}x{height}, {fps}fps, {total_frames}å¸§")

        # è¾“å‡ºæ–‡ä»¶
        video_name = Path(input_video).stem
        timestamp = datetime.now().strftime("%H%M%S")
        output_video = Path(output_dir) / f"{video_name}_tracking_{timestamp}.mp4"
        output_sample = Path(output_dir) / f"{video_name}_sample_{timestamp}.jpg"
        output_trails = Path(output_dir) / f"{video_name}_trails_{timestamp}.jpg" if save_trails else None

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        writer = cv2.VideoWriter(str(output_video), cv2.VideoWriter_fourcc(*"mp4v"), fps, (width, height))

        # è½¨è¿¹ç”»å¸ƒ
        trails_canvas = np.zeros((height, width, 3), dtype=np.uint8) if save_trails else None

        frame_count = 0
        total_detections = 0
        total_tracks = 0
        process_frames = min(max_frames or total_frames, total_frames)
        sample_saved = False
        track_history = {}

        while frame_count < process_frames:
            ret, frame = cap.read()
            if not ret:
                break

            # é«˜çº§æ£€æµ‹
            detections = detector.detect_with_details(
                frame=frame,
                target_class=target_class if target_class != "all" else None,
                confidence_threshold=confidence_threshold,
                max_detections=max_detections,
                min_box_area=min_box_area,
                max_box_area=max_box_area,
                return_format="detailed",
                filter_overlapping=True,
                overlap_threshold=0.5,
                sort_by="confidence",
            )

            # é«˜çº§è·Ÿè¸ªæ›´æ–°
            tracks = tracker.update(
                detections=detections,
                frame_id=frame_count,
                track_confidence_threshold=track_confidence_threshold,
                association_metric=association_metric,
                enable_kalman_filter=enable_kalman_filter,
            )

            # æ›´æ–°è½¨è¿¹å†å²
            for track_id, track_data in tracks.items():
                if track_id not in track_history:
                    track_history[track_id] = []

                if isinstance(track_data, dict) and "bbox" in track_data:
                    bbox = track_data["bbox"]
                    center = ((bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2)
                    track_history[track_id].append(center)

                    # é™åˆ¶è½¨è¿¹é•¿åº¦
                    if len(track_history[track_id]) > trail_length:
                        track_history[track_id] = track_history[track_id][-trail_length:]

            # é«˜çº§å¯è§†åŒ–ï¼ˆè°ƒæ•´å‚æ•°åç§°ä»¥åŒ¹é…å‡½æ•°ç­¾åï¼‰
            def get_color_for_track(track_id):
                """æ ¹æ®é¢œè‰²æ–¹æ¡ˆè·å–è½¨è¿¹é¢œè‰²"""
                if color_scheme == "id_based":
                    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
                    return colors[track_id % len(colors)]
                elif color_scheme == "confidence_based":
                    # åŸºäºç½®ä¿¡åº¦çš„é¢œè‰²ï¼ˆéœ€è¦ä»tracksè·å–ç½®ä¿¡åº¦ï¼‰
                    return (0, 255, 0)  # é»˜è®¤ç»¿è‰²
                else:
                    return (255, 255, 255)  # é»˜è®¤ç™½è‰²

            annotated = visualize_tracking(
                frame=frame,
                tracks=tracks,
                trails=track_history if show_trails else {},
                trails_canvas=trails_canvas,
                get_color_func=get_color_for_track,
                frame_info=f"Frame: {frame_count+1}/{process_frames}",
                show_trail=show_trails,
                show_velocity=False,  # è¿™é‡Œä½¿ç”¨falseï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡å®ç°é€Ÿåº¦è®¡ç®—
                show_track_id=show_track_id,
                trail_thickness=trail_thickness,
                fade_trail=fade_trail,
                color_per_track=(color_scheme == "id_based"),
            )

            # ä¿å­˜
            writer.write(annotated)
            total_detections += len(detections)
            total_tracks += len(tracks)

            # ä¿å­˜ç¤ºä¾‹å¸§
            if not sample_saved and frame_count == process_frames // 2:
                cv2.imwrite(str(output_sample), annotated)
                sample_saved = True

            frame_count += 1

            # è¿›åº¦
            if frame_count % 50 == 0:
                progress = frame_count / process_frames * 100
                active_tracks = len([t for t in tracks.values() if isinstance(t, dict) and t.get("state") == "active"])
                print(
                    f"â³ è¿›åº¦: {progress:.1f}% ({frame_count}/{process_frames}) - "
                    f"æ£€æµ‹: {len(detections)}, æ´»è·ƒè½¨è¿¹: {active_tracks}"
                )

        cap.release()
        writer.release()

        # ä¿å­˜è½¨è¿¹å›¾
        if save_trails and trails_canvas is not None:
            cv2.imwrite(str(output_trails), trails_canvas)

        # ç»Ÿè®¡
        stats = {
            "input_video": input_video,
            "output_video": str(output_video),
            "output_sample": str(output_sample),
            "output_trails": str(output_trails) if output_trails else None,
            "processed_frames": frame_count,
            "total_detections": total_detections,
            "total_tracks": total_tracks,
            "unique_track_ids": len(track_history),
            "avg_detections": total_detections / frame_count if frame_count > 0 else 0,
            "avg_tracks": total_tracks / frame_count if frame_count > 0 else 0,
            "detection_settings": {
                "confidence_threshold": confidence_threshold,
                "target_class": target_class,
                "max_detections": max_detections,
                "min_box_area": min_box_area,
                "max_box_area": max_box_area,
            },
            "tracking_settings": {
                "track_confidence_threshold": track_confidence_threshold,
                "association_metric": association_metric,
                "enable_kalman_filter": enable_kalman_filter,
                "max_age": max_age,
                "min_hits": min_hits,
                "iou_threshold": iou_threshold,
            },
            "visualization_settings": {
                "trail_length": trail_length,
                "color_scheme": color_scheme,
                "fade_trail": fade_trail,
                "trail_thickness": trail_thickness,
            },
            "timestamp": timestamp,
        }

        # ä¿å­˜ç»Ÿè®¡
        if save_stats:
            stats_file = Path(output_dir) / f"{video_name}_stats_{timestamp}.json"
            with open(stats_file, "w") as f:
                json.dump(stats, f, indent=2)
            print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {stats_file}")

        print("âœ… è·Ÿè¸ªå®Œæˆ!")
        print(f"ğŸ“¹ è·Ÿè¸ªè§†é¢‘: {output_video}")
        print(f"ğŸ“¸ ç¤ºä¾‹å›¾ç‰‡: {output_sample}")
        if output_trails:
            print(f"ğŸ›¤ï¸ è½¨è¿¹å›¾ç‰‡: {output_trails}")
        print(f"ğŸ“ˆ å¤„ç†äº† {frame_count} å¸§")
        print(f"ğŸ” æ€»æ£€æµ‹æ•°: {total_detections} (å¹³å‡ {total_detections/frame_count:.1f}/å¸§)")
        print(f"ğŸ¯ æ€»è·Ÿè¸ªæ•°: {total_tracks} (å¹³å‡ {total_tracks/frame_count:.1f}/å¸§)")
        print(f"ğŸ·ï¸ å”¯ä¸€IDæ•°: {len(track_history)}")

        return stats

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()
