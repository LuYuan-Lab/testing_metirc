"""
è§†é¢‘è£å‰ªå¤„ç†å™¨
æ•´åˆäº†è‡ªåŠ¨è£å‰ªæ£€æµ‹å’Œæ‰¹é‡è§†é¢‘å¤„ç†åŠŸèƒ½
"""

import json
import os
from typing import Optional, Tuple

import cv2
import numpy as np
from ultralytics import YOLO


class AutoCropper:
    """
    ä½¿ç”¨ YOLO æ£€æµ‹ç›®æ ‡åŒºåŸŸå¹¶è¿”å›è£å‰ªæ¡†ï¼Œæ”¯æŒå¤šç§è°ƒç”¨æ¨¡å¼
    """

    def __init__(
        self,
        model_path: str = "weights/yolov11n.pt",
        conf_thres: float = 0.5,
        target_class=None,
        margin_ratio: float = 0.1,
        default_crop_rect: Tuple[int, int, int, int] = (720, 120, 1700, 1000),
        max_missing_frames: int = 30,
    ):
        """
        Args:
            model_path: YOLO æ¨¡å‹è·¯å¾„æˆ–æ¨¡å‹å
            conf_thres: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
            target_class: æƒ³è¦æ£€æµ‹çš„ç±»åˆ«ï¼Œå¯ä»¥æ˜¯ç±»åˆ«ID(int) æˆ– åç§°(str)ï¼Œå¦‚ 'person'
            margin_ratio: æ‰©å¤§ bbox çš„æ¯”ä¾‹
            default_crop_rect: æ£€æµ‹å¤±è´¥æ—¶ä½¿ç”¨çš„é»˜è®¤è£å‰ªåŒºåŸŸ
            max_missing_frames: è¿ç»­å¤šå°‘å¸§æœªæ£€æµ‹åˆ°ç›®æ ‡åï¼Œåˆ¤å®šä¸ºçœŸæ­£ä¸¢å¤±
        """
        try:
            self.model = YOLO(model_path)
        except FileNotFoundError:
            print(f"âš ï¸ æ¨¡å‹ {model_path} æœªæ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨ YOLOv8n æ›¿ä»£")
            self.model = YOLO("yolov11n.pt")

        self.model.fuse()
        self.model.conf = conf_thres
        self.target_class = target_class
        self.margin_ratio = margin_ratio
        self.default_crop_rect = default_crop_rect
        self.max_missing_frames = max_missing_frames

        # çŠ¶æ€ç¼“å­˜
        self.last_crop_rect: Optional[Tuple[int, int, int, int]] = None
        self.missing_count: int = 0

        # âœ… æ˜ å°„ç±»åˆ«ååˆ° ID
        if isinstance(self.target_class, str):
            names = self.model.names
            name_to_id = {v: k for k, v in names.items()}
            if self.target_class in name_to_id:
                mapped_id = name_to_id[self.target_class]
                print(f"âœ… æ£€æµ‹ç›®æ ‡ '{self.target_class}' -> ç±»åˆ«ID {mapped_id}")
                self.target_class = mapped_id
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°ç±»åˆ«å '{self.target_class}'ï¼Œå°†æ£€æµ‹æ‰€æœ‰ç±»åˆ«ã€‚")
                self.target_class = None

    def detect_crop_rect(self, frame: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """
        è¾“å…¥ä¸€å¸§ RGB å›¾åƒï¼Œè¿”å›è£å‰ªæ¡† (x1, y1, x2, y2)
        è‹¥è¿ç»­è¶…è¿‡ max_missing_frames å¸§æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œåˆ™è¿”å› None
        """
        results = self.model(frame, verbose=False)
        boxes = results[0].boxes

        if boxes is None or len(boxes) == 0:
            self.missing_count += 1
            if self.last_crop_rect is not None and self.missing_count < self.max_missing_frames:
                return self.last_crop_rect
            else:
                return None

        cls_ids = boxes.cls.cpu().numpy().astype(int)
        xyxy_all = boxes.xyxy.cpu().numpy()

        # åªä¿ç•™ç›®æ ‡ç±»åˆ«
        if self.target_class is not None:
            mask = cls_ids == self.target_class
            if np.any(mask):
                xyxy_all = xyxy_all[mask]
            else:
                self.missing_count += 1
                if self.last_crop_rect is not None and self.missing_count < self.max_missing_frames:
                    return self.last_crop_rect
                else:
                    return None

        # æ‰¾é¢ç§¯æœ€å¤§ç›®æ ‡
        areas = (xyxy_all[:, 2] - xyxy_all[:, 0]) * (xyxy_all[:, 3] - xyxy_all[:, 1])
        largest_idx = np.argmax(areas)
        x1, y1, x2, y2 = map(int, xyxy_all[largest_idx])

        # æ·»åŠ  margin
        w, h = x2 - x1, y2 - y1
        x1 = max(0, int(x1 - w * self.margin_ratio))
        y1 = max(0, int(y1 - h * self.margin_ratio))
        x2 = int(x2 + w * self.margin_ratio)
        y2 = int(y2 + h * self.margin_ratio)

        crop_rect = (x1, y1, x2, y2)

        # æ›´æ–°çŠ¶æ€
        self.last_crop_rect = crop_rect
        self.missing_count = 0
        return crop_rect

    def detect_video_crop(self, video_path: str) -> Tuple[int, int, int, int]:
        """
        å¯¹è§†é¢‘å‰å‡ å¸§æ£€æµ‹ç›®æ ‡ï¼Œè¿”å›ç»Ÿä¸€è£å‰ªæ¡†
        è‹¥æ‰€æœ‰å¸§éƒ½æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œåˆ™è¿”å› default_crop_rect
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âš ï¸ æ— æ³•æ‰“å¼€è§†é¢‘ {video_path}ï¼Œä½¿ç”¨é»˜è®¤è£å‰ªæ¡†")
            return self.default_crop_rect

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_frames = min(5, total_frames)

        crop_rects = []
        for idx in range(sample_frames):
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if not ret:
                continue
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            rect = self.detect_crop_rect(rgb_frame)
            if rect is not None:
                crop_rects.append(rect)

        cap.release()

        if not crop_rects:
            return self.default_crop_rect

        # åˆå¹¶æœ‰æ•ˆæ£€æµ‹æ¡†
        x1 = min([r[0] for r in crop_rects])
        y1 = min([r[1] for r in crop_rects])
        x2 = max([r[2] for r in crop_rects])
        y2 = max([r[3] for r in crop_rects])

        return x1, y1, x2, y2

    def detect_with_details(
        self,
        frame: np.ndarray,
        target_class: str = None,
        confidence_threshold: float = None,
        max_detections: int = None,
        min_box_area: float = 100,
        max_box_area: float = None,
        crop_margin: float = 0,
        return_format: str = "detailed",
        filter_overlapping: bool = True,
        overlap_threshold: float = 0.5,
        sort_by: str = "confidence",
        enable_nms: bool = True,
        nms_threshold: float = 0.4,
    ):
        """
        è¯¦ç»†çš„æ£€æµ‹æ¥å£ï¼Œè¿”å›ä¸°å¯Œçš„æ£€æµ‹ä¿¡æ¯

        Args:
            frame: è¾“å…¥å¸§
            target_class: ç›®æ ‡ç±»åˆ«åç§°ï¼ŒNoneåˆ™æ£€æµ‹æ‰€æœ‰
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ŒNoneåˆ™ä½¿ç”¨æ¨¡å‹é»˜è®¤
            max_detections: æœ€å¤§æ£€æµ‹æ•°é‡
            min_box_area: æœ€å°æ£€æµ‹æ¡†é¢ç§¯
            max_box_area: æœ€å¤§æ£€æµ‹æ¡†é¢ç§¯
            crop_margin: è£å‰ªè¾¹è·åƒç´ 
            return_format: è¿”å›æ ¼å¼ ["simple", "detailed", "full"]
            filter_overlapping: æ˜¯å¦è¿‡æ»¤é‡å æ¡†
            overlap_threshold: é‡å é˜ˆå€¼
            sort_by: æ’åºæ–¹å¼ ["confidence", "area", "position"]
            enable_nms: æ˜¯å¦å¯ç”¨NMS
            nms_threshold: NMSé˜ˆå€¼

        Returns:
            æ ¹æ®return_formatè¿”å›ä¸åŒæ ¼å¼çš„æ£€æµ‹ç»“æœ
        """
        # è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼
        if confidence_threshold is not None:
            original_conf = self.model.conf
            self.model.conf = confidence_threshold

        try:
            # æ‰§è¡Œæ£€æµ‹
            results = self.model(frame, verbose=False)
            boxes = results[0].boxes

            if boxes is None or len(boxes) == 0:
                return []

            # è·å–æ£€æµ‹æ•°æ®
            cls_ids = boxes.cls.cpu().numpy().astype(int)
            xyxy_all = boxes.xyxy.cpu().numpy()
            conf_all = boxes.conf.cpu().numpy()

            detections = []

            for i, (box, cls_id, conf) in enumerate(zip(xyxy_all, cls_ids, conf_all)):
                class_name = self.model.names[cls_id]

                # ç½®ä¿¡åº¦è¿‡æ»¤ - å…³é”®ä¿®å¤ï¼
                if confidence_threshold is not None and conf < confidence_threshold:
                    continue

                # ç±»åˆ«è¿‡æ»¤
                if target_class and class_name != target_class:
                    continue

                x1, y1, x2, y2 = map(int, box)

                # é¢ç§¯è¿‡æ»¤
                area = (x2 - x1) * (y2 - y1)
                if area < min_box_area:
                    continue
                if max_box_area and area > max_box_area:
                    continue

                # æ·»åŠ è£å‰ªè¾¹è·
                if crop_margin > 0:
                    x1 = max(0, x1 - crop_margin)
                    y1 = max(0, y1 - crop_margin)
                    x2 = min(frame.shape[1], x2 + crop_margin)
                    y2 = min(frame.shape[0], y2 + crop_margin)

                # æ„å»ºæ£€æµ‹ç»“æœ
                detection = {
                    "bbox": (x1, y1, x2, y2),
                    "confidence": float(conf),
                    "class": class_name,
                    "class_id": int(cls_id),
                    "area": area,
                    "center": ((x1 + x2) // 2, (y1 + y2) // 2),
                }

                if return_format == "full":
                    detection.update(
                        {"width": x2 - x1, "height": y2 - y1, "aspect_ratio": (x2 - x1) / (y2 - y1), "detection_id": i}
                    )

                detections.append(detection)

            # æ’åº
            if sort_by == "confidence":
                detections.sort(key=lambda x: x["confidence"], reverse=True)
            elif sort_by == "area":
                detections.sort(key=lambda x: x["area"], reverse=True)
            elif sort_by == "position":
                detections.sort(key=lambda x: (x["center"][1], x["center"][0]))

            # é™åˆ¶æ•°é‡
            if max_detections:
                detections = detections[:max_detections]

            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            if return_format == "simple":
                return [(d["bbox"], d["confidence"], d["class"]) for d in detections]
            elif return_format == "detailed":
                return detections
            else:  # full
                return detections

        finally:
            # æ¢å¤åŸå§‹ç½®ä¿¡åº¦
            if confidence_threshold is not None:
                self.model.conf = original_conf

    def detect_and_crop(
        self,
        frame: np.ndarray,
        target_class: str = "person",
        confidence_threshold: float = None,
        max_detections: int = None,
        min_box_area: float = 100,
        crop_margin: float = 0,
        return_crops: bool = False,
    ):
        """
        æ£€æµ‹å¹¶è¿”å›è£å‰ªåæ ‡ï¼Œä¸ºæ•°æ®é›†è°ƒç”¨ä¼˜åŒ–

        Args:
            frame: è¾“å…¥å¸§
            target_class: ç›®æ ‡ç±»åˆ«
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            max_detections: æœ€å¤§æ£€æµ‹æ•°é‡
            min_box_area: æœ€å°æ£€æµ‹æ¡†é¢ç§¯
            crop_margin: è£å‰ªè¾¹è·
            return_crops: æ˜¯å¦è¿”å›è£å‰ªåçš„å›¾åƒ

        Returns:
            å¦‚æœreturn_crops=False: è¿”å›åæ ‡åˆ—è¡¨ [(x1,y1,x2,y2), ...]
            å¦‚æœreturn_crops=True: è¿”å› (åæ ‡åˆ—è¡¨, è£å‰ªå›¾åƒåˆ—è¡¨)
        """
        detections = self.detect_with_details(
            frame=frame,
            target_class=target_class,
            confidence_threshold=confidence_threshold,
            max_detections=max_detections,
            min_box_area=min_box_area,
            crop_margin=crop_margin,
            return_format="simple",
        )

        # æå–åæ ‡
        crop_boxes = [det[0] for det in detections]

        if not return_crops:
            return crop_boxes

        # ç”Ÿæˆè£å‰ªå›¾åƒ
        crops = []
        for x1, y1, x2, y2 in crop_boxes:
            crop = frame[y1:y2, x1:x2]
            crops.append(crop)

        return crop_boxes, crops


class VideoCropProcessor:
    """
    æ‰¹é‡è§†é¢‘è£å‰ªå¤„ç†å™¨
    """

    def __init__(
        self,
        model_path: str = "weights/yolov11n.pt",
        conf_thres: float = 0.5,
        target_class: str = "person",
        margin_ratio: float = 0.1,
    ):
        """
        åˆå§‹åŒ–è§†é¢‘è£å‰ªå¤„ç†å™¨

        Args:
            model_path: YOLO æ¨¡å‹è·¯å¾„
            conf_thres: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
            target_class: æ£€æµ‹ç›®æ ‡ç±»åˆ«
            margin_ratio: è£å‰ªæ¡†æ‰©å±•æ¯”ä¾‹
        """
        self.cropper = AutoCropper(
            model_path=model_path,
            conf_thres=conf_thres,
            target_class=target_class,
            margin_ratio=margin_ratio,
        )

    def generate_crop_boxes(self, data_root: str, output_json: str):
        """
        éå†æ•´ä¸ªæ•°æ®é›†ï¼Œä¸ºæ¯ä¸ªè§†é¢‘æ£€æµ‹äººæ¡†å¹¶ä¿å­˜åˆ° JSONã€‚

        Args:
            data_root: æ•°æ®é›†æ ¹ç›®å½•
            output_json: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        """
        crop_dict = {}

        # éå† train/val ç­‰æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for root, _, files in os.walk(data_root):
            for f in files:
                if f.lower().endswith((".mp4", ".avi", ".mov", ".mkv")):
                    video_path = os.path.join(root, f)
                    try:
                        crop_rect = self.cropper.detect_video_crop(video_path)
                        crop_dict[video_path] = crop_rect
                        print(f"âœ… {video_path} -> {crop_rect}")
                    except Exception as e:
                        print(f"âš ï¸ Failed to process {video_path}: {e}")

        # ä¿å­˜åˆ° JSON æ–‡ä»¶
        output_dir = os.path.dirname(output_json)
        if output_dir:  # åªæœ‰å½“ç›®å½•è·¯å¾„ä¸ä¸ºç©ºæ—¶æ‰åˆ›å»ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
        with open(output_json, "w") as fp:
            json.dump(crop_dict, fp, indent=4)
        print(f"\nğŸ¯ Saved {len(crop_dict)} crop boxes to {output_json}")
        return crop_dict

    def process_single_video(self, video_path: str) -> Tuple[int, int, int, int]:
        """
        å¤„ç†å•ä¸ªè§†é¢‘ï¼Œè¿”å›è£å‰ªæ¡†

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            è£å‰ªæ¡†åæ ‡ (x1, y1, x2, y2)
        """
        return self.cropper.detect_video_crop(video_path)

    def load_crop_boxes(self, json_path: str) -> dict:
        """
        ä»JSONæ–‡ä»¶åŠ è½½è£å‰ªæ¡†æ•°æ®

        Args:
            json_path: JSONæ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«è§†é¢‘è·¯å¾„å’Œè£å‰ªæ¡†çš„å­—å…¸
        """
        try:
            with open(json_path, "r") as fp:
                crop_dict = json.load(fp)
            print(f"âœ… ä» {json_path} åŠ è½½äº† {len(crop_dict)} ä¸ªè£å‰ªæ¡†")
            return crop_dict
        except FileNotFoundError:
            print(f"âš ï¸ æ–‡ä»¶ {json_path} ä¸å­˜åœ¨")
            return {}
        except json.JSONDecodeError:
            print(f"âš ï¸ JSON æ–‡ä»¶ {json_path} æ ¼å¼é”™è¯¯")
            return {}


def main():
    """
    ä¸»å‡½æ•° - æ‰¹é‡å¤„ç†æ•°æ®é›†ä¸­çš„æ‰€æœ‰è§†é¢‘
    """
    # é…ç½®å‚æ•°
    data_root = "data"  # ä½ çš„æ•°æ®æ ¹ç›®å½•
    output_json = "boxes_json/crop_boxes.json"

    # åˆ›å»ºå¤„ç†å™¨
    processor = VideoCropProcessor(
        model_path="weights/yolov11n.pt", conf_thres=0.5, target_class="person", margin_ratio=0.1
    )

    # ç”Ÿæˆè£å‰ªæ¡†
    crop_boxes = processor.generate_crop_boxes(data_root, output_json)

    print(f"\nğŸ“Š å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(crop_boxes)} ä¸ªè§†é¢‘æ–‡ä»¶")


if __name__ == "__main__":
    main()
