"""
ç¨³å¥çš„è§†é¢‘åˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import os
import time
from tqdm import tqdm

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from dataset import VideoDataset
from model import R2Plus1DNet
from loss import ImprovedTripletLoss, ArcFaceLoss, CombinedLoss, CenterLoss
from loss_config import get_loss_config, get_training_config, list_available_configs

# å¯¼å…¥æ•°æ®é¢„å¤„ç†ç±»
import random
from PIL import Image

class CustomCropWithNoise:
    def __init__(self, left=1200, top=250, width=1300, height=1500, noise_range=50):
        self.left = left
        self.top = top
        self.width = width
        self.height = height
        self.noise_range = noise_range
    
    def __call__(self, img):
        img_width, img_height = img.size
        
        # æ£€æŸ¥å›¾åƒå°ºå¯¸æ˜¯å¦è¶³å¤Ÿå¤§
        if img_width < self.width or img_height < self.height:
            # å¦‚æœå›¾åƒå¤ªå°ï¼Œç›´æ¥resizeåˆ°ç›®æ ‡å°ºå¯¸
            return img.resize((self.width, self.height))
        
        # æ·»åŠ éšæœºå™ªå£°æŠ–åŠ¨
        noise_x = random.randint(-self.noise_range, self.noise_range)
        noise_y = random.randint(-self.noise_range, self.noise_range)
        
        # è®¡ç®—å®é™…è£å‰ªåæ ‡ï¼Œç¡®ä¿ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
        actual_left = max(0, min(self.left + noise_x, img_width - self.width))
        actual_top = max(0, min(self.top + noise_y, img_height - self.height))
        actual_right = min(img_width, actual_left + self.width)
        actual_bottom = min(img_height, actual_top + self.height)
        
        return img.crop((actual_left, actual_top, actual_right, actual_bottom))

class CustomCropWithoutNoise:
    def __init__(self, left=1200, top=250, width=1300, height=1500):
        self.left = left
        self.top = top
        self.width = width
        self.height = height
    
    def __call__(self, img):
        img_width, img_height = img.size
        
        # æ£€æŸ¥å›¾åƒå°ºå¯¸æ˜¯å¦è¶³å¤Ÿå¤§
        if img_width < self.width or img_height < self.height:
            return img.resize((self.width, self.height))
        
        actual_left = max(0, min(self.left, img_width - self.width))
        actual_top = max(0, min(self.top, img_height - self.height))
        actual_right = min(img_width, actual_left + self.width)
        actual_bottom = min(img_height, actual_top + self.height)
        
        return img.crop((actual_left, actual_top, actual_right, actual_bottom))

def create_data_loaders(data_dir, batch_size=4, num_frames=16, image_size=224, config=None):
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨
    """
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # ä»é…ç½®è·å–è£å‰ªå‚æ•°
    if config is None:
        config = get_main_training_config()
    
    CROP_LEFT = config['CROP_LEFT']
    CROP_TOP = config['CROP_TOP']
    CROP_WIDTH = config['CROP_WIDTH']
    CROP_HEIGHT = config['CROP_HEIGHT']
    NOISE_RANGE = config['NOISE_RANGE']

    # éªŒè¯é›†å˜æ¢
    val_transform = transforms.Compose([
        CustomCropWithoutNoise(
            left=CROP_LEFT, 
            top=CROP_TOP, 
            width=CROP_WIDTH, 
            height=CROP_HEIGHT
        ),
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # è®­ç»ƒé›†å˜æ¢
    train_transform = transforms.Compose([
        CustomCropWithNoise(
            left=CROP_LEFT, 
            top=CROP_TOP, 
            width=CROP_WIDTH, 
            height=CROP_HEIGHT,
            noise_range=NOISE_RANGE
        ),
        transforms.Resize((image_size, image_size)),
        transforms.RandomHorizontalFlip(p=config['HORIZONTAL_FLIP_PROB']),
        transforms.ColorJitter(
            brightness=config['BRIGHTNESS'], 
            contrast=config['CONTRAST'], 
            saturation=config['SATURATION'], 
            hue=config['HUE']
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = VideoDataset(
        root_dir=os.path.join(data_dir, 'train'),
        num_frames=num_frames,
        transform=train_transform
    )

    val_dataset = VideoDataset(
        root_dir=os.path.join(data_dir, 'val'),
        num_frames=num_frames,
        transform=val_transform
    )

    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  ç±»åˆ«: {train_dataset.classes}")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"  éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨é…ç½®å‚æ•°
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=config['NUM_WORKERS'],
        pin_memory=config['PIN_MEMORY'],
        drop_last=config['DROP_LAST_TRAIN']
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=config['NUM_WORKERS'],
        pin_memory=config['PIN_MEMORY'],
        drop_last=config['DROP_LAST_VAL']
    )

    return train_loader, val_loader, train_dataset.classes

def train_epoch(model, train_loader, criterion, optimizer, device, epoch, total_epochs, config):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    """
    model.train()
    running_loss = 0.0
    running_loss_components = {}  # ç”¨äºè®°å½•ç»„åˆæŸå¤±çš„å„ä¸ªç»„ä»¶
    num_batches = len(train_loader)
    
    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{total_epochs} - Training')
    
    for batch_idx, (videos, labels) in enumerate(progress_bar):
        try:
            videos = videos.to(device)
            labels = labels.to(device)
            
            # å‰å‘ä¼ æ’­
            embeddings = model(videos)
            
            # å¤„ç†ä¸åŒæŸå¤±å‡½æ•°çš„è¿”å›å€¼
            if isinstance(criterion, (ArcFaceLoss,)):
                loss, logits = criterion(embeddings, labels)
            elif isinstance(criterion, CombinedLoss):
                loss, loss_dict, logits = criterion(embeddings, labels)
            else:  # ImprovedTripletLoss or other single return loss
                loss = criterion(embeddings, labels)
                logits = None
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å€¼
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"è­¦å‘Š: æ£€æµ‹åˆ°æ— æ•ˆæŸå¤±å€¼ {loss.item()}, è·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['GRADIENT_CLIP_MAX_NORM'])
            
            optimizer.step()
            
            running_loss += loss.item()
            
            # è®°å½•ç»„åˆæŸå¤±çš„å„ä¸ªç»„ä»¶
            if isinstance(criterion, CombinedLoss) and 'loss_dict' in locals():
                for component_name, component_loss in loss_dict.items():
                    if component_name not in running_loss_components:
                        running_loss_components[component_name] = 0.0
                    running_loss_components[component_name] += component_loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            postfix_dict = {
                'Loss': f'{loss.item():.4f}',
                'Avg Loss': f'{running_loss/(batch_idx+1):.4f}'
            }
            
            # æ·»åŠ ç»„ä»¶æŸå¤±ä¿¡æ¯åˆ°è¿›åº¦æ¡
            if running_loss_components:
                for name, comp_loss in running_loss_components.items():
                    postfix_dict[f'{name.capitalize()}'] = f'{comp_loss/(batch_idx+1):.4f}'
            
            progress_bar.set_postfix(postfix_dict)
            
        except Exception as e:
            print(f"æ‰¹æ¬¡ {batch_idx} è®­ç»ƒæ—¶å‡ºé”™: {e}")
            continue
    
    return running_loss / max(num_batches, 1)

def validate_epoch(model, val_loader, criterion, device, epoch, total_epochs, config):
    """
    éªŒè¯ä¸€ä¸ªepoch
    """
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    
    progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{total_epochs} - Validation')
    
    with torch.no_grad():
        for batch_idx, (videos, labels) in enumerate(progress_bar):
            try:
                videos = videos.to(device)
                labels = labels.to(device)
                
                # å‰å‘ä¼ æ’­
                embeddings = model(videos)
                
                # å¤„ç†ä¸åŒæŸå¤±å‡½æ•°çš„è¿”å›å€¼
                if isinstance(criterion, (ArcFaceLoss,)):
                    loss, logits = criterion(embeddings, labels)
                elif isinstance(criterion, CombinedLoss):
                    loss, loss_dict, logits = criterion(embeddings, labels)
                else:  # ImprovedTripletLoss or other single return loss
                    loss = criterion(embeddings, labels)
                    logits = None
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å€¼
                if not (torch.isnan(loss) or torch.isinf(loss)):
                    running_loss += loss.item()
                
                # å‡†ç¡®ç‡è®¡ç®— - æ ¹æ®æŸå¤±å‡½æ•°ç±»å‹é€‰æ‹©ä¸åŒçš„è®¡ç®—æ–¹å¼
                batch_size = embeddings.size(0)
                
                if isinstance(criterion, (ArcFaceLoss, CombinedLoss)) and logits is not None:
                    # å¯¹äºæœ‰åˆ†ç±»logitsçš„æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨åˆ†ç±»å‡†ç¡®ç‡
                    _, predicted = torch.max(logits, 1)
                    correct_predictions += (predicted == labels).sum().item()
                    total_samples += labels.size(0)
                    
                elif batch_size > 1:
                    # å¯¹äºåº¦é‡å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨æœ€è¿‘é‚»å‡†ç¡®ç‡
                    for i in range(batch_size):
                        query_embedding = embeddings[i:i+1]
                        query_label = labels[i]
                        
                        # è®¡ç®—ä¸å…¶ä»–æ ·æœ¬çš„è·ç¦»
                        distances = torch.cdist(query_embedding, embeddings)
                        distances[0, i] = float('inf')  # æ’é™¤è‡ªå·±
                        
                        if distances[0].numel() > 0:
                            nearest_idx = torch.argmin(distances)
                            nearest_label = labels[nearest_idx]
                            
                            if nearest_label == query_label:
                                correct_predictions += 1
                        total_samples += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                accuracy = (correct_predictions / max(total_samples, 1)) * 100
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Avg Loss': f'{running_loss/(batch_idx+1):.4f}',
                    'Acc': f'{accuracy:.2f}%'
                })
                
            except Exception as e:
                print(f"æ‰¹æ¬¡ {batch_idx} éªŒè¯æ—¶å‡ºé”™: {e}")
                continue
    
    avg_loss = running_loss / max(len(val_loader), 1)
    accuracy = (correct_predictions / max(total_samples, 1)) * 100
    
    return avg_loss, accuracy

def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    # ========== è½½å…¥å’ŒéªŒè¯é…ç½®å‚æ•° ==========
    config = get_main_training_config()
    validate_config(config)
    
    print("å¼€å§‹è®­ç»ƒè§†é¢‘åˆ†ç±»æ¨¡å‹...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è·å–é…ç½®
    print("\n" + "="*50)
    loss_config = get_loss_config(config['LOSS_CONFIG_NAME'])
    training_config = get_training_config(config['TRAINING_CONFIG_NAME'])
    print("="*50)
    
    # ä»é…ç½®è·å–å‚æ•°
    DATA_DIR = config['DATA_DIR']
    NUM_CLASSES = config['NUM_CLASSES']
    EMBEDDING_DIM = training_config['embedding_dim']
    BATCH_SIZE = training_config['batch_size']
    NUM_FRAMES = training_config['num_frames']
    IMAGE_SIZE = training_config['image_size']
    LEARNING_RATE = training_config['learning_rate']
    NUM_EPOCHS = training_config['num_epochs']
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, classes = create_data_loaders(
        DATA_DIR, 
        batch_size=BATCH_SIZE,
        num_frames=NUM_FRAMES,
        image_size=IMAGE_SIZE,
        config=config
    )
    
    print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = R2Plus1DNet(
        num_classes=NUM_CLASSES,
        embedding_dim=EMBEDDING_DIM,
        pretrained=True
    ).to(device)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    print("åˆ›å»ºæŸå¤±å‡½æ•°...")
    
    # ä»é…ç½®åˆ›å»ºæŸå¤±å‡½æ•°
    LOSS_TYPE = loss_config['type']
    loss_params = loss_config['params']
    
    if LOSS_TYPE == 'triplet':
        criterion = ImprovedTripletLoss(**loss_params)
        
    elif LOSS_TYPE == 'arcface':
        criterion = ArcFaceLoss(
            in_features=EMBEDDING_DIM,
            out_features=NUM_CLASSES,
            **loss_params
        )
        
    elif LOSS_TYPE == 'combined':
        criterion = CombinedLoss(
            in_features=EMBEDDING_DIM,
            out_features=NUM_CLASSES,
            **loss_params
        )
    
    # ç¡®ä¿æŸå¤±å‡½æ•°ä¹Ÿç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
    criterion = criterion.to(device)
    print(f"æŸå¤±å‡½æ•°å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
    
    print(f"ä½¿ç”¨æŸå¤±å‡½æ•°: {LOSS_TYPE}")
    
    # æ‰“å°æŸå¤±å‡½æ•°è¯¦ç»†ä¿¡æ¯
    if LOSS_TYPE == 'triplet':
        print(f"  - Triplet Losså‚æ•°: margin={criterion.margin}, mining={criterion.mining_strategy}, metric={criterion.distance_metric}")
    elif LOSS_TYPE == 'arcface':
        print(f"  - ArcFaceå‚æ•°: s={criterion.s}, m={criterion.m}, easy_margin={criterion.easy_margin}")
    elif LOSS_TYPE == 'combined':
        print(f"  - ç»„åˆæŸå¤±æƒé‡: {criterion.loss_weights}")
        print(f"  - ArcFaceå‚æ•°: s={criterion.arcface_loss.s}, m={criterion.arcface_loss.m}")
        print(f"  - Tripletå‚æ•°: margin={criterion.triplet_loss.margin}, mining={criterion.triplet_loss.mining_strategy}")
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=config['WEIGHT_DECAY'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config['LR_SCHEDULER_STEP_SIZE'], gamma=config['LR_SCHEDULER_GAMMA'])
    
    # è®­ç»ƒè®°å½•
    train_losses = []
    val_losses = []
    val_accuracies = []
    best_accuracy = 0.0
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {NUM_EPOCHS} ä¸ªepoch...")
    
    for epoch in range(NUM_EPOCHS):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
        print(f"{'='*60}")
        
        try:
            # è®­ç»ƒ
            start_time = time.time()
            train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch, NUM_EPOCHS, config)
            train_time = time.time() - start_time
            
            # éªŒè¯
            start_time = time.time()
            val_loss, val_accuracy = validate_epoch(model, val_loader, criterion, device, epoch, NUM_EPOCHS, config)
            val_time = time.time() - start_time
            
        except KeyboardInterrupt:
            print(f"\nç”¨æˆ·ä¸­æ–­è®­ç»ƒåœ¨epoch {epoch+1}")
            print("ä¿å­˜å½“å‰è¿›åº¦...")
            # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'criterion_state_dict': criterion.state_dict() if hasattr(criterion, 'state_dict') else None,
                'optimizer_state_dict': optimizer.state_dict(),
                'train_losses': train_losses,
                'val_losses': val_losses,
                'val_accuracies': val_accuracies,
                'interrupted': True
            }, 'interrupted_model.pth')
            print("æ¨¡å‹å·²ä¿å­˜ä¸º: interrupted_model.pth")
            break
            
        except Exception as e:
            print(f"Epoch {epoch+1} å‡ºç°é”™è¯¯: {e}")
            print("ç»§ç»­ä¸‹ä¸€ä¸ªepoch...")
            continue
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•ç»“æœ
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)
        
        # è¾“å‡ºç»“æœ
        print(f"\nEpoch {epoch+1} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} (è€—æ—¶: {train_time:.1f}s)")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f} (è€—æ—¶: {val_time:.1f}s)")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%")
        print(f"  å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'criterion_state_dict': criterion.state_dict() if hasattr(criterion, 'state_dict') else None,
                'optimizer_state_dict': optimizer.state_dict(),
                'best_accuracy': best_accuracy,
                'train_loss': train_loss,
                'val_loss': val_loss,
                'val_accuracy': val_accuracy,
                'classes': classes,
                'loss_type': LOSS_TYPE,
                'hyperparameters': {
                    'embedding_dim': EMBEDDING_DIM,
                    'num_classes': NUM_CLASSES,
                    'batch_size': BATCH_SIZE,
                    'learning_rate': LEARNING_RATE
                }
            }, config['BEST_MODEL_PATH'])
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {best_accuracy:.2f}%)")
        
        # æ¯Nä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        if (epoch + 1) % config['SAVE_CHECKPOINT_EVERY'] == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'criterion_state_dict': criterion.state_dict() if hasattr(criterion, 'state_dict') else None,
                'optimizer_state_dict': optimizer.state_dict(),
                'train_losses': train_losses,
                'val_losses': val_losses,
                'val_accuracies': val_accuracies,
                'classes': classes,
                'loss_type': LOSS_TYPE
            }, f'checkpoint_epoch_{epoch+1}.pth')
            print(f"  âœ“ ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_epoch_{epoch+1}.pth")
    
    print(f"\n{'='*60}")
    print("è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.2f}%")
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º: {config['BEST_MODEL_PATH']}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    torch.save({
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_accuracies': val_accuracies,
        'classes': classes,
        'loss_type': LOSS_TYPE,
        'hyperparameters': {
            'batch_size': BATCH_SIZE,
            'num_frames': NUM_FRAMES,
            'image_size': IMAGE_SIZE,
            'learning_rate': LEARNING_RATE,
            'num_epochs': NUM_EPOCHS,
            'embedding_dim': EMBEDDING_DIM,
            'loss_type': LOSS_TYPE
        }
    }, config['TRAINING_HISTORY_PATH'])
    print(f"è®­ç»ƒå†å²å·²ä¿å­˜ä¸º: {config['TRAINING_HISTORY_PATH']}")

def get_main_training_config():
    """
    ========================================================================
                            ğŸ”§ è®­ç»ƒé…ç½®ä¸­å¿ƒ ğŸ”§
    ========================================================================
    
    è¿™é‡ŒåŒ…å«äº†æ‰€æœ‰å¯ä»¥è°ƒæ•´çš„è®­ç»ƒå‚æ•°ï¼Œæ‚¨åªéœ€è¦ä¿®æ”¹è¿™ä¸ªå‡½æ•°ä¸­çš„å‚æ•°å€¼
    å°±å¯ä»¥è°ƒæ•´æ•´ä¸ªè®­ç»ƒæµç¨‹ã€‚ä¸å†éœ€è¦åœ¨ä»£ç å„å¤„ä¿®æ”¹å‚æ•°ï¼
    
    ğŸ“‹ ä¸»è¦é…ç½®ç±»åˆ«:
    - åŸºç¡€é…ç½®: æ•°æ®è·¯å¾„ã€ç±»åˆ«æ•°é‡
    - æŸå¤±å‡½æ•°é…ç½®: é€‰æ‹©ä¸åŒçš„æŸå¤±å‡½æ•°ç»„åˆ
    - æ•°æ®é¢„å¤„ç†: è£å‰ªã€å¢å¼ºç­‰å‚æ•°
    - è®­ç»ƒè¶…å‚æ•°: å­¦ä¹ ç‡ã€æƒé‡è¡°å‡ç­‰
    - æ¨¡å‹ä¿å­˜: æ–‡ä»¶è·¯å¾„å’Œä¿å­˜é¢‘ç‡
    
    ğŸ’¡ å¿«é€Ÿé…ç½®æŒ‡å—:
    1. ä¿®æ”¹ LOSS_CONFIG_NAME æ¥é€‰æ‹©æŸå¤±å‡½æ•°
    2. ä¿®æ”¹ TRAINING_CONFIG_NAME æ¥é€‰æ‹©è®­ç»ƒå¼ºåº¦
    3. è°ƒæ•´ CROP_* å‚æ•°æ¥é€‚åº”æ‚¨çš„å›¾åƒ
    4. æ ¹æ®éœ€è¦è°ƒæ•´å…¶ä»–å‚æ•°
    
    ========================================================================
    """
    config = {
        # ========== åŸºç¡€é…ç½® ==========
        'DATA_DIR': './data_',                    # æ•°æ®ç›®å½•
        'NUM_CLASSES': 5,                        # ç±»åˆ«æ•°é‡
        
        # ========== æŸå¤±å‡½æ•°é…ç½® ==========
        # ğŸ¯ æ¨èé…ç½®ç»„åˆ:
        # - 'behavior_recognition' + 'small_memory'  â­ è¡Œä¸ºè¯†åˆ«æ¨è (ç»„åˆæŸå¤±)
        # - 'combined_balanced' + 'default'          â­ å¹³è¡¡æ€§èƒ½ (ç»„åˆæŸå¤±)
        # - 'combined_classification' + 'default'     ğŸ¯ å¼ºè°ƒåˆ†ç±» (ç»„åˆæŸå¤±)
        # - 'small_dataset' + 'small_memory'          ğŸ’¾ å°æ•°æ®é›† (ç»„åˆæŸå¤±)
        # - 'triplet' + 'fast_prototype'              âš¡ çº¯ä¸‰å…ƒç»„è®­ç»ƒ
        
        # å½“å‰é…ç½®: æ¸è¿›å¼å¹³è¡¡æŸå¤± (æ¸©å’Œè°ƒæ•´æƒé‡)
        'LOSS_CONFIG_NAME': 'behavior_progressive',
        
        # å…¶ä»–å¯é€‰é…ç½®:
        # 'LOSS_CONFIG_NAME': 'behavior_recognition_enhanced',  # å¢å¼ºç‰ˆï¼ˆä¸­ç­‰å¹³è¡¡ï¼‰
        # 'LOSS_CONFIG_NAME': 'behavior_balanced',              # å¼ºå¹³è¡¡ç‰ˆï¼ˆæ¿€è¿›è°ƒæ•´ï¼‰
        # 'LOSS_CONFIG_NAME': 'behavior_recognition',           # åŸå§‹é…ç½®ï¼ˆæƒé‡è¾ƒå°ï¼‰
        # 'LOSS_CONFIG_NAME': 'behavior_metric_learning',       # å¼ºè°ƒåº¦é‡å­¦ä¹ 
        # 'LOSS_CONFIG_NAME': 'triplet',                        # çº¯ä¸‰å…ƒç»„è®­ç»ƒ
        
        # ========== è®­ç»ƒå‚æ•°é…ç½® ==========
        # å¯é€‰: 'default', 'large_batch', 'small_memory', 'fast_prototype'
        'TRAINING_CONFIG_NAME': 'default',
        
        # ========== æ•°æ®é¢„å¤„ç†å‚æ•° ==========
        'CROP_LEFT': 1200,                       # è£å‰ªå·¦è¾¹ç•Œ
        'CROP_TOP': 250,                         # è£å‰ªä¸Šè¾¹ç•Œ
        'CROP_WIDTH': 1300,                      # è£å‰ªå®½åº¦
        'CROP_HEIGHT': 1500,                     # è£å‰ªé«˜åº¦
        'NOISE_RANGE': 50,                       # è®­ç»ƒæ—¶çš„éšæœºå™ªå£°èŒƒå›´
        
        # ========== æ•°æ®å¢å¼ºå‚æ•° ==========
        'HORIZONTAL_FLIP_PROB': 0.5,            # æ°´å¹³ç¿»è½¬æ¦‚ç‡
        'BRIGHTNESS': 0.2,                       # äº®åº¦è°ƒæ•´èŒƒå›´
        'CONTRAST': 0.2,                         # å¯¹æ¯”åº¦è°ƒæ•´èŒƒå›´
        'SATURATION': 0.1,                       # é¥±å’Œåº¦è°ƒæ•´èŒƒå›´
        'HUE': 0.1,                             # è‰²è°ƒè°ƒæ•´èŒƒå›´
        
        # ========== è®­ç»ƒè¶…å‚æ•° ==========
        'WEIGHT_DECAY': 1e-4,                   # æƒé‡è¡°å‡
        'GRADIENT_CLIP_MAX_NORM': 1.0,          # æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°
        'LR_SCHEDULER_STEP_SIZE': 5,            # å­¦ä¹ ç‡è°ƒåº¦æ­¥é•¿
        'LR_SCHEDULER_GAMMA': 0.5,              # å­¦ä¹ ç‡è¡°å‡ç³»æ•°
        
        # ========== æ¨¡å‹ä¿å­˜é…ç½® ==========
        'SAVE_CHECKPOINT_EVERY': 5,             # æ¯Nä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
        'BEST_MODEL_PATH': 'best_model.pth',    # æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
        'TRAINING_HISTORY_PATH': 'training_history.pth',  # è®­ç»ƒå†å²ä¿å­˜è·¯å¾„
        
        # ========== æ•°æ®åŠ è½½å™¨é…ç½® ==========
        'NUM_WORKERS': 0,                       # Windowså…¼å®¹è®¾ç½®
        'PIN_MEMORY': False,                    # å†…å­˜å›ºå®š
        'DROP_LAST_TRAIN': True,               # è®­ç»ƒæ—¶ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´batch
        'DROP_LAST_VAL': False,                # éªŒè¯æ—¶ä¸ä¸¢å¼ƒ
        
        # ========== æ˜¾ç¤ºå’Œæ—¥å¿—é…ç½® ==========
        'VERBOSE': True,                        # è¯¦ç»†è¾“å‡º
        'PROGRESS_BAR': True,                   # æ˜¾ç¤ºè¿›åº¦æ¡
    }
    
    return config

def validate_config(config):
    """
    éªŒè¯é…ç½®çš„å®Œæ•´æ€§å’Œåˆç†æ€§
    """
    # æ£€æŸ¥å¿…éœ€çš„é…ç½®é¡¹
    required_keys = [
        'DATA_DIR', 'NUM_CLASSES', 'LOSS_CONFIG_NAME', 'TRAINING_CONFIG_NAME',
        'CROP_LEFT', 'CROP_TOP', 'CROP_WIDTH', 'CROP_HEIGHT'
    ]
    
    missing_keys = [key for key in required_keys if key not in config]
    if missing_keys:
        raise ValueError(f"é…ç½®ç¼ºå°‘å¿…éœ€é¡¹: {missing_keys}")
    
    # æ£€æŸ¥æ•°å€¼çš„åˆç†æ€§
    if config['NUM_CLASSES'] <= 0:
        raise ValueError("NUM_CLASSES å¿…é¡»å¤§äº0")
    
    if config['CROP_WIDTH'] <= 0 or config['CROP_HEIGHT'] <= 0:
        raise ValueError("è£å‰ªå°ºå¯¸å¿…é¡»å¤§äº0")
    
    if not os.path.exists(config['DATA_DIR']):
        print(f"è­¦å‘Š: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {config['DATA_DIR']}")
    
    print("âœ“ é…ç½®éªŒè¯é€šè¿‡")

if __name__ == "__main__":
    main()